[{"authors":["admin"],"categories":null,"content":"I am a researcher in quantitative criminology at the Crime and Well-being Big Data Centre, a research institute based at Manchester Metropolitan University.\nMy research focuses on describing and explaining the geographic patterning of crime. I am also interested in data visualisation, mapping and promoting the use of open software in social science.\nThis website provides an overview of my ongoing work, but for a complete summary please check my CV.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/samuel-langton/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/samuel-langton/","section":"authors","summary":"I am a researcher in quantitative criminology at the Crime and Well-being Big Data Centre, a research institute based at Manchester Metropolitan University.\nMy research focuses on describing and explaining the geographic patterning of crime.","tags":null,"title":"Samuel Langton","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e87efdfe209d90ea3c6332a7cbd9d08b","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"\rOver the past few weeks I have spent a bit of time exploring police recorded crime trends before and after nationwide lockdowns induced by the COVID19 pandemic. Some of this has involved making descriptive graphics across different crime types, using London (UK) as a case study.\nIn this post, I will take you through how you can create these visuals in R. It’s worth emphasising that this is simply a demonstration of ‘how I get did’ not ‘how to do it’ – there will be many ways of doing the same thing, some probably more beautiful and concise.\nBackground\rThere has been talk of lockdowns representing the largest natural experiment in criminological history. Of course, this kind of claim will be challenged, especially when demonstrations involve police recorded crime data. Lots of parameters will have changed beyond people’s routine activities, amongst them, people’s ability and willingness to report crimes to the police, and policing resource allocation. It might be some time before we can fully understand how lockdowns have curbed the spatial and temporal patterns of crime.\nIn the meantime, police recorded crime data can offer a unique insight into how criminal behaviour might have changed in the past few months. In the UK, open data is released on a monthly basis. This isn’t ideal by any means, but it does allow for some preliminary exploration. Here, we’re going to create a descriptive visualisation inspired by some infamous graphs by the FT used to track COVID19 deaths.\nOne powerful thing about these visuals is that they effortlessly account for seasonal trends in death rates using previous years as a reference point. Readers can then intuitively understand what is ‘normal’, and identify fluctuations which are irregular, and thus likely a result of COVID19.\nSimilarly, a key characteristic of long-term crime trends is seasonality. To disentangle fluctuations in crime, and pinpoint irregularity resulting from lockdown measures, we must account for typical monthly fluctuations. We can do this descriptively (and visually) using open police recorded crime data and some packages in R.\nFor now, we will focus on the base visual – perhaps in further posts I can demonstrate how to more accurately mimic the ‘FT look’. We’ll aim for something like the graphic below, representing multiple crime type trends in London from January 2016 to May 2020.\n\rProject and packages\rTo make following this tutorial easier, some of you might want to download the associated repository on GitHub. By working within the R project on the repository, and using the same folder structure, the subsequent code won’t need to be edited. Any data we download should just be placed into the data folder, and named accordingly. Whatever works for you!\nAlthough we are mainly using ggplot2 for the visual, we use a number of other packages for data wrangling and plot arrangement. Load them using library() now. If you don’t have some of these packages, ensure they are installed using install.packages().\nlibrary(readr)\rlibrary(dplyr)\rlibrary(tidyr)\rlibrary(ggplot2)\rlibrary(cowplot)\rlibrary(stringr)\r\rOpen crime data\rFirst, we need the data. There are several ways to download open police data in the UK. There is an API, an R package, and a direct download page from the website.\nHere, we are going to get most of the data through direct download from the online data portal. Remembering that we want previous years as our reference point, we are going to select the widest date range possible (at the time of writing), which is the 36 months spanning from June 2017 to May 2020. As new data is released, this 36-month window will shift. So for example, once June 2020 becomes available, you will only be able to download July 2017 to June 2020. This isn’t a huge deal, but remember to take that into account when following subsequent steps.\nYou can select the latest 36-month range using the dropdown menus, and then tick whatever police force(s) we want. In this example, I am going to use the Metropolitan Police Service, which serve Greater London. So, our selection will look something like this:\nWe can then ‘Generate file’ at the bottom of the page, and on the next page, download a .zip file containing all our crime data for the past 36 months. Here, I call the .zip file met_police_june2017_to_june2020 and save it to a local folder named data, as per the associated GitHub repository.\nIdeally, I’d like to go even further back, before June 2017, to get the best picture possible about seasonal crime trends. We can do this using the open archive data. This comes in a pretty unfriendly format, because you cannot download by force, and cannot select specific time periods. Anyway, you can download all the data prior to June 2017 using a direct download from the archive page and the following option:\nRemember that if your 36-month window is more recent than this demonstration, you will need to select a different corresponding archive folder! I name mine archive_to_jun_2017 and save it in the same data folder. Note that this might take 5-10 minutes to download, depending on your internet connection. Once unzipped, you should have two folders in your data folder, named accordingly. Mine look like this:\n\rLoading the data\rYou will notice that, once unzipped, data for the last 36 months (named met_police_june2017_to_june2020) is spread across multiple folders – one for each month. This is not ideal, but using some tricks in R we can automatically load these files and bind them together into one data frame.\nFirst, let’s generate a list containing all the working directories and .csv files in the downloaded folder. Note that we are taking the location and name of the folder relative to where our R project is saved (\"data/met_police_june2017_to_june2020/\"), extracting the name of every .csv file contained within it, and pasting them together.\n# Extract list of working directories + file names for each csv.\rfile_names \u0026lt;- paste(\u0026quot;data/met_police_june2017_to_june2020/\u0026quot;, list.files(\u0026quot;data/met_police_june2017_to_june2020\u0026quot;, \u0026quot;*.csv\u0026quot;, recursive=TRUE), sep = \u0026quot;\u0026quot;)\rPrinting the object file_names to your Console will provide you with the working directory and file name of the .csv files – of which there are 36 – contained within met_police_june2017_to_june2020. We can then load all of these .csv files into R by looping the read_csv() function from readr through the file_names object.\n# Loop through each to load.\rmet_list \u0026lt;- lapply(file_names, read_csv)\rOkay, we are nearly there. We can complete the missing months for 2017 using the archive material. Again, remember that you might need additional months depending on the 36-month time window you’re using. Here, we only need January to May, so rather than use another loop, let’s just load them individually, assigning them to meaningful object names.\njan17_df \u0026lt;- read_csv(\u0026quot;data/archive_to_jun_2017/2017-01/2017-01-metropolitan-street.csv\u0026quot;)\rfeb17_df \u0026lt;- read_csv(\u0026quot;data/archive_to_jun_2017/2017-02/2017-02-metropolitan-street.csv\u0026quot;) mar17_df \u0026lt;- read_csv(\u0026quot;data/archive_to_jun_2017/2017-03/2017-03-metropolitan-street.csv\u0026quot;) apr17_df \u0026lt;- read_csv(\u0026quot;data/archive_to_jun_2017/2017-04/2017-04-metropolitan-street.csv\u0026quot;) may17_df \u0026lt;- read_csv(\u0026quot;data/archive_to_jun_2017/2017-05/2017-05-metropolitan-street.csv\u0026quot;) \rFinally (the interesting bit is coming, I promise), let’s extract the monthly data for 2016 from the archive data, so we have the complete set from 2016-2020. We do this like the first example, but with a few extra options to ensure that we select data from the Metropolitan Police only, rather than every police force. Remember that the archive data automatically contains data from every police force – but we only want the Met.\n# Extract 2016 for the Met only.\rdates_2016 \u0026lt;- list.files(\u0026quot;data/archive_to_jun_2017\u0026quot;, pattern = \u0026quot;2016\u0026quot;)\rextra \u0026lt;- paste(\u0026quot;data/archive_to_jun_2017\u0026quot;, \u0026quot;/\u0026quot;, dates_2016, \u0026quot;/\u0026quot;, dates_2016, \u0026quot;-\u0026quot;, \u0026quot;metropolitan-street.csv\u0026quot;, sep = \u0026quot;\u0026quot;)\r# Load in loop.\rmet_2016_list \u0026lt;- lapply(extra, read_csv)\rNow, at last, we can bind all these data together row-wise using bind_rows(). The resulting object met_full_df contains all records for the Metropolitan Police from Janury 2016 to May 2020. Okay, it’s been a bit of palavar, but we have managed to download and compile over 4.5 million crime records with (relative) ease. Hurrah!\n# Bind these to existing data frame objects.\rmet_full_df \u0026lt;- bind_rows(met_2016_list, jan17_df, feb17_df, mar17_df, apr17_df, may17_df, met_recent_df)\r\rVisual plot\rWhat we have currently are individual records of crime recorded by the Met from January 2016 to May 2020. We can aggregate these records by month, year and crime type using a series of functions in dplyr along with the piping operator %\u0026gt;%. Note the steps taken. We first filter out ‘other crimes’ – a crude decision simply because I don’t think this crime type offers much insight. We then create separate month and year variables, group by months, years and crime type, count the number of crimes for each combination, arrange for easy viewing, and then ungroup. The final step splits the data frame into a list of data frames by each crime type, so we can easily generate a plot for each crime type later.\n# Counts.\rmet_stats_df \u0026lt;- met_full_df %\u0026gt;% # select the data frame\rfilter(`Crime type` != \u0026quot;Other crime\u0026quot;) # filter out other crimes\rseparate(Month, into = c(\u0026quot;Year\u0026quot;,\u0026quot;Month\u0026quot;), sep = \u0026quot;-\u0026quot;) %\u0026gt;% # create new \u0026#39;month\u0026#39; and \u0026#39;year\u0026#39; variables\rgroup_by(Year, Month, `Crime type`) %\u0026gt;% # group by year, month and crime type\rsummarise(counts = n()) %\u0026gt;% # count rows per group combination\rarrange(`Crime type`, Year, Month) %\u0026gt;% # sort data frame for viewing with View()\rungroup() %\u0026gt;% # ungroup data frame\rsplit_group(`Crime type`) # split into lists of df by crime type\rWe can then create the ggplot function which will be used to generate the plots for each crime type.\n# Base visual function.\rplot_fun \u0026lt;- function(x){\rggplot() +\rgeom_line(data = filter(x, Year != 2020),\rmapping = aes(x = Month, y = counts, group = Year),\rcolour = \u0026quot;lightgrey\u0026quot;) +\rgeom_vline(data = x,\rmapping = aes(xintercept = 2.7),\rlinetype = \u0026quot;dotted\u0026quot;) + stat_summary(data = filter(x, Year != 2020),\raes(x = Month, y = counts, group = Year),\rfun = mean, colour = \u0026quot;white\u0026quot;, geom = \u0026quot;line\u0026quot;, group=1, size = 0.8) +\rstat_summary(data = filter(x, Year != 2020),\raes(x = Month, y = counts, group = Year),\rfun = mean, colour = \u0026quot;black\u0026quot;, geom = \u0026quot;line\u0026quot;, group=1, size = 0.7) +\rgeom_line(data = filter(x, Year == 2020),\rmapping = aes(x = Month, y = counts, group = 1),\rcolour = \u0026quot;white\u0026quot;, size = 1.1) +\rgeom_line(data = filter(x, Year == 2020),\rmapping = aes(x = Month, y = counts, group = 1),\rcolour = \u0026quot;tomato\u0026quot;, size = 1) +\rscale_x_discrete(labels = c(\u0026quot; \u0026quot;,\u0026quot;Feb\u0026quot;,\u0026quot;Mar\u0026quot;,\u0026quot;Apr\u0026quot;,\u0026quot;May\u0026quot;,\u0026quot;Jun\u0026quot;,\u0026quot;Jul\u0026quot;,\u0026quot;Aug\u0026quot;,\u0026quot;Sep\u0026quot;,\u0026quot;Oct\u0026quot;,\u0026quot;Nov\u0026quot;,\u0026quot;Dec\u0026quot;)) +\rscale_y_continuous(limits = c(0.6*min(x$counts), 1.4*max(x$counts))) +\rlabs(x = NULL, y = NULL) +\rtheme_bw() +\rtheme(axis.title = element_blank(),\raxis.text.x = element_blank(),\raxis.text.y = element_text(size = 6),\raxis.ticks = element_blank(),\rplot.title = element_text(size = 8)) }\rThis function can then be looped through met_stats_list, the list of data frames by crime type. Note that we first name each data frame in the list so that we can add titles automatically. In doing so, we also remove the underscores. The resulting object met_stats_plots is a list of ggplot objects, visualising crime trends for each crime type.\n# Create a character vector naming each crime type, which helps us automatically add titles to our plots.\rnames(met_stats_list) \u0026lt;- sort(unique(met_full_df$`Crime type`))\r# Make plots and add titles.\rmet_stats_plots \u0026lt;- list()\rfor (i in seq(met_stats_list)) {\rgg \u0026lt;- plot_fun(met_stats_list[[i]]) +\rlabs(title = str_replace_all(string = names(met_stats_list[i]), \u0026quot;_\u0026quot;, \u0026quot; \u0026quot;))\rmet_stats_plots[[i]] \u0026lt;- gg\r}\rWe can arrange the list of plots using the plot_grid() function from cowplot to get a basic summary of crime trends in London.\n# Plot grid.\rplot_grid(plotlist = met_stats_plots, ncol = 3)\r\rAnnotating and arranging\rThe problem with the above graph is the lack of annotation. Rather than add a legend, we can simply annotate one of these plots. The starkest trend is probably anti-social behaviour, so let’s retrieve this plot from our list.\n# Pluck out ASB for annotation.\rAnti_social_behaviour \u0026lt;- pluck(met_stats_plots, 1)\rWe can then add arrows and text labels using annotate(). This seems rather fiddly at first (because it is) but once you’re used to it, stating accurate x and y arguments is quite straightforward. Note that by specifying axis.text.x we bring back the x-axis annotation we used in the original function, specifying the months.\n# Annotate.\rasb \u0026lt;- Anti_social_behaviour +\rtheme(axis.text.x = element_text(size = 6, angle = 90, vjust = -1.5)) +\rlabs(title = \u0026quot;Anti-social behaviour\u0026quot;) +\r#2020 label\rannotate(geom = \u0026quot;curve\u0026quot; , x = 7, y = 73000, xend = 5, yend = 61000,\rcurvature = 0.3, arrow = arrow(length = unit(1, \u0026quot;mm\u0026quot;))) +\rannotate(geom = \u0026quot;text\u0026quot;, x = 7.8, y = 73000, label = \u0026quot;2020\u0026quot;, fontface = \u0026quot;bold\u0026quot;, size = 3) +\r#lockdown label\rannotate(geom = \u0026quot;curve\u0026quot;, x = 4, y = 9000, xend = 2.8, yend = 9000,\rcurvature = 0.1, arrow = arrow(length = unit(1, \u0026quot;mm\u0026quot;))) +\rannotate(geom = \u0026quot;text\u0026quot;, x = 5.5, y = 9000, label = \u0026quot;Lockdown\u0026quot;, fontface = \u0026quot;bold\u0026quot;, size = 3) +\r#recent years label\rannotate(geom = \u0026quot;curve\u0026quot;, x = 8, y = 38500, xend = 8.3, yend = 26000,\rcurvature = -0.1, arrow = arrow(length = unit(1, \u0026quot;mm\u0026quot;))) +\rannotate(geom = \u0026quot;text\u0026quot;, x = 8, y = 43000, label = \u0026quot;Recent years (inc. mean)\u0026quot;, fontface = \u0026quot;bold\u0026quot;, size = 3) \rNow we’ve made a separate (annotated) plot for anti-social behaviour, we can get rid of the one contained in our original list.\n# Remove ASB for plot list to avoid repeat.\rmet_stats_plots[[1]] \u0026lt;- NULL\rThen comes the arranging. Here, I use the plot_grid function from the amusingly named cowplot package. Again, this is quite fiddly – an alternative might be patchwork, but I am quite familiar with cowplot already. Note that we actually create two separate cowplots – one for anti-social behaviour, and one for our main plots – and then stick them together.\nasb_blank \u0026lt;- plot_grid(asb, NULL, nrow = 1)\rall_plots \u0026lt;- plot_grid(plotlist = met_stats_plots, nrow = 4)\rfull_plot \u0026lt;- plot_grid(asb_blank, all_plots, nrow = 2, rel_heights = c(2,5))\rYou can then print the plot to your Viewer or save it.\n# Save\rggsave(full_plot, filename = \u0026quot;visuals/full_met.png\u0026quot;, height = 8, width = 6)\r\rConclusion\r\r","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"24ab3115520fdc6d27b27520404eb079","permalink":"/post/tutorial1_crime_covid/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/post/tutorial1_crime_covid/","section":"post","summary":"Over the past few weeks I have spent a bit of time exploring police recorded crime trends before and after nationwide lockdowns induced by the COVID19 pandemic. Some of this has involved making descriptive graphics across different crime types, using London (UK) as a case study.","tags":["policing","crime","COVID","R","ggplot2"],"title":"Tutorial: visualising lockdown crime trends","type":"post"},{"authors":null,"categories":null,"content":"Introduction to QGIS Developed and taught a 1-day introductory course at the Cathie Marsh Institute (University of Manchester). Material is available on GitHub.\nA shorter introductory course has been held online for Open Data Manchester, with an accompanying video. Material is avaiable on GitHub.\n","date":1593216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593216000,"objectID":"74c688af6ed1c96a9abfe2811bde5a5f","permalink":"/project/qgis-teaching/","publishdate":"2020-06-27T00:00:00Z","relpermalink":"/project/qgis-teaching/","section":"project","summary":"QGIS course(s).","tags":["QGIS","mapping","GIS","visualisation","teaching"],"title":"QGIS","type":"project"},{"authors":null,"categories":null,"content":"I have taught a number of courses using R and RStudio, including the development of teaching material both on my own and with colleagues.\nData visualisation in R using crime data 1-day workshop on visualisation and mapping at the Cathie Marsh Institute (University of Manchester) in collaboration with the UK Data Service. Material is available on GitHub and the UK Data Service website.\nR for police forces 1-day workshop on data wrangling and visualisation for West Midlands Police Force. Please get in contact for material.\nR for criminologists 1-day workshop on data wrangling, visualisation and mapping hosted at the European Society of Criminology with colleagues from the Space Place and Crime working group. Material is available on GitHub and rpubs.\nGetting Started in R - an introduction to data analysis and visualisation 4-day comprehensive introductory course at the Methods@Manchester summer school, co-taught with colleagues. Material is available as a website.\nCluster analysis in R 1-day workshop introducing cluster analysis in R at the Cathie Marsh Institute (University of Manchester), co-taught with a colleage. Please get in contact for material.\nR for social scientists I am a qualified instructor for Data and Software Carpentries, and supported colleagues in teaching the social science workshop.\n","date":1593216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593216000,"objectID":"3dcf67677fd4025bf9369cd49793cf33","permalink":"/project/r-teaching/","publishdate":"2020-06-27T00:00:00Z","relpermalink":"/project/r-teaching/","section":"project","summary":"R course(s).","tags":["R","RStudio","teaching"],"title":"R","type":"project"},{"authors":null,"categories":null,"content":"\rUnderstanding what has happened to crime during lockdown is challenging. We are in uncharted territory and it’s proving hard to draw definitive conclusions from the latest police recorded crime data. But a few trends, like a spike in antisocial behaviour and an increase in drug offences, are beginning to emerge.\nChanges in people’s movements and activities can impact on the opportunities available to potential offenders and, in turn, the volume and types of crime being committed. As a case study, we can look at Greater London and consider crimes recorded by the Metropolitan Police Service before and after the nationwide lockdown.\nUsing this data, I have visualised how the Met recorded antisocial behaviour and different crime types on a month-by-month basis. Importantly, there is enough open data to compare trends across years. This is useful for studying crime because many offence types are seasonal.\nFor instance, bicycle theft tends to increase in spring (around when the lockdown began) so looking at 2020 in isolation would be uninformative, or even misleading. This mimics the approach taken when visually tracking coronvirus fatalities due to the seasonality of death rates.\nCrimes such as burglary, shoplifting, robbery and theft all experienced remarkably sudden declines during April in comparison to previous years. Although we cannot differentiate individual days and weeks, some of these declines were already evident in March.\nThis aligns with what many will have expected. With most people spending so much time at home, shopping limited to essentials, and movement restricted to necessary travel and exercise, the opportunities available to commit such offences were severely curtailed.\nAnti-social behaviour\rOne of the starkest trends observed is antisocial behaviour. Even by the end of March, recorded incidences were unusually high. But between March and April, the number of cases skyrocketed by over 270%. An increase at this time of year is expected, but a rise of this magnitude is unprecedented.\nIt is plausible that the lockdown resulted in a genuine shift in “traditional” antisocial behaviour, such as nuisance noise from neighbours. But the National Police Chiefs’ Council has said that this increase can largely be attributed to people breaching lockdown guidelines.\nThe picture is not so clear for criminal offences. There has not yet been any discernible impact on public order or the possession of weapons. Violence and sexual offences appear to have declined more steeply than usual.\nHowever, using these charts in isolation, it is difficult to definitively attribute these changes to the lockdown. Even during less remarkable times, police recorded crime patterns can be subject to short-term blips. Indeed, visualising the long-term trend for violence and sexual offences demonstrates that similarly volatile changes have occurred before.\nDrug offences appear to have gone up in comparison to previous years. Addiction experts have speculated that the lockdown could result in an increase in drug usage among recreational users. At the same time, some police forces have suggested that the demand for drugs has remained the same, but that drug dealers now stick out on otherwise empty streets, increasing the number of arrests.\nIn other words, these changes may not necessarily reflect actual changes in criminal behaviour. The long-term picture, visualised below, demonstrates that April’s increase might also be part of a wider upward trend in Greater London. The rise in recent years might be partly attributable to action taken over county lines drug networks.\n\rExercising caution\rSome have labelled the nationwide lockdown as an ideal natural experiment to study how a shift in the opportunities available to offenders might impact on crime. But of course, it is not that simple.\nNumerous parameters have changed. Recording practices for lockdown breaches have had to be adapted overnight. The willingness and ability of people to report crimes to the police, particularly for domestic offences, will have changed in ways that cannot yet be quantified. Calls to a national domestic abuse helpline have increased, but open police records do not flag which violent and sexual offences, for instance, were domestic.\nPolicing priorities have also had to be adjusted in response to vague government guidelines and there has been considerable strain on officers as a result.\nOne way of overcoming shortcomings in police data is by using victimisation surveys such as the Crime Survey for England and Wales that can capture crimes not reported or identified by the police. This largely resolves issues around under-reporting and includes additional crime types such as cybercrime which might have increased during the lockdown. However, the survey data is not month-by-month and will not be released for some time.\nFor now, open data in the UK represents a unique opportunity to track police recorded crime – as long as it is used with a degree of caution.\n\r","date":1591920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591920000,"objectID":"c2cb3cfc0d028302741c86810fe0df76","permalink":"/post/lockdown_conversation/","publishdate":"2020-06-12T00:00:00Z","relpermalink":"/post/lockdown_conversation/","section":"post","summary":"Understanding what has happened to crime during lockdown is challenging. We are in uncharted territory and it’s proving hard to draw definitive conclusions from the latest police recorded crime data.","tags":["crime","policing","covid","lockdown","London"],"title":"Lockdown crime trends. What we know and what we don't","type":"post"},{"authors":["David Buil-Gil","Angelo Moretti","Samuel Langton"],"categories":null,"content":"","date":1591488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591488000,"objectID":"5a569bf2d1ef67b6f607c9475ff58a7e","permalink":"/publication/crimsim_preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/crimsim_preprint/","section":"publication","summary":"Paper using simulated data to investigate bias in police recorded crime at multiple spatial scales.","tags":["policing","crime","data bias","reporting","simulation"],"title":"The integrity of crime statistics. Assessing the impact of police data bias on crime mapping","type":"publication"},{"authors":["Monsuru Adepeju","Samuel Langton","Jon Bannister"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"2b259935ef42f9faf77e03b6a3653c91","permalink":"/publication/akmedoid/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/akmedoid/","section":"publication","summary":"R package to implement anchored k-medoids, a method for clustering longitudinal data.","tags":["crime","clustering","kmeans","longitudinal","R"],"title":"Akmedoids. Anchored kmedoids for longitudinal data clustering","type":"publication"},{"authors":["David Buil-Gil","Samuel Langton"],"categories":null,"content":"","date":1575676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575676800,"objectID":"a8f2f85821952fd147e4f3325dc85fa1","permalink":"/publication/gis_chapter/","publishdate":"2019-12-07T00:00:00Z","relpermalink":"/publication/gis_chapter/","section":"publication","summary":"Forthcoming book chapter introducing GIS and geovisual analysis for research.","tags":["GIS","mapping","visualisation"],"title":"GIS and Geovisual Analysis","type":"publication"},{"authors":null,"categories":null,"content":"\rFrom reporting election results to issuing weather forecasts, maps offer a powerful, accessible and visually appealing way to convey complex information. Yet even the most beautiful maps can introduce some degree of misrepresentation.\nTo see how, consider the latest statistics on deprivation released by the UK government. The government ranked 32,844 neighbourhoods, based on measures of deprivation such as income, employment, health and crime. The figures were widely reported, from the BBC to The Guardian and the Daily Mail, reigniting long-standing debates about persistent inequality in England.\nOf course, many outlets used maps to share these findings with the public. But using traditional boundaries can divert readers’ attention away from important information. In BBC’s map of deprivation across local authorities, for instance, sparsely populated rural areas dominate a disproportionately large area, while urban areas, such as London, containing millions of people, are rendered almost invisible.\nThe map included in the BBC’s report on deprivation is interesting. It would be good to take a look at the Ofsted rating of schools in the most deprived areas... https://t.co/iUR2hINBRn pic.twitter.com/lFq6s8l1In\n\u0026mdash; SailingAway (@Heatherleatt) September 27, 2019  Distorted data\rRecent research shows that people can interpret information inaccurately, when they look at maps with these shortcomings. But other techniques can be used to create a more accurate impression of the underlying data. For instance, cartograms deliberately distort geography by scaling areas according to a specific variable, such as local population.\nDorling cartograms take this one step further, scaling areas according to a specified variable, but also representing each area using the same shape, such as a square or a circle. Other methods achieve uniformity in both size and shape: hexograms and geogrids transform the original boundaries of the map into hexagons or squares of the same size, while still aiming to preserve their original arrangement.\nMapping neighbourhood deprivation in England represents a significant challenge, even for experts. This is because the government defines a neighbourhood as a “lower super output area” – each of which contains around 1,500 residents.\nBecause highly deprived neighbourhoods tend to be densely populated, they are less visible on a regular map. By contrast, wealthier suburban areas – which are often less densely populated and therefore much larger – dominate the map. So there’s a risk that readers might draw inaccurate conclusions about the level of deprivation in any given area.\n\rShaping up\rUsing Dorling cartograms scaled by resident population, and regular hexagonal geogrids, I’ve attempted to minimise any misrepresentation. Take the example of Blackpool, which was ranked the most deprived local authority in England. Around 42% of neighbourhoods in Blackpool are in the top 10% most deprived in England (the “first decile”). Yet these neighbourhoods only make up around 29% of the city’s actual area.\nUsing a Dorling cartogram scaled by resident population size, neighbourhoods in the first decile now take up 41% of the map’s area, and the larger, less deprived neighbourhoods (in light blue) have been shrunk accordingly to become less dominant. With a regular hexagonal geogrid, the map now mimics the underlying data, with neighbourhoods in the first decile of deprivation taking up 42% of the total area.\nThe variation in the shapes and sizes of Blackpool’s neighbourhoods can no longer divert attention or mislead readers, and arguably, the underlying data has been conveyed more accurately than with the original map.\n\rMapping deprivation differently\rA striking difference can be seen using different types of maps to visualise deprivation in Burnley and Hartlepool, which were among England’s most deprived local authorities. Both areas are characterised by many small, densely populated neighbourhoods in the first decile (most deprived), surrounded by much larger and wealthier suburbs.\nAt first glance, deprivation appears understated in the map that uses the original boundaries. But with the Dorling cartogram (again scaled by population) and the hexagonal geogrids, the issue can be somewhat rectified. This comes with its own difficulties, though.\nThe geogrid in particular pushed some neighbourhoods further apart and forced some closer together than they appear on the original map. This might prove problematic in cases where the primary aim of the map is to convey distinct geographic patterns, such as the clustering of highly deprived neighbourhoods. In making the transformation, these patterns might be lost, or spurious patterns generated.\n\rEncouraging experimentation\rBirmingham and Manchester – ranked in the top ten most deprived local authorities – present a different challenge. With much larger populations, and many more neighbourhoods to visualise, the original map presents an overwhelming amount of information.\nOnce again, both the Dorling cartogram and the geogrid perform admirably, scaling down the visual impact of larger, wealthier areas (especially in the north of Birmingham) and making the maps easier to read – without skewing the geographic patterning of deprivation across each city.\nThere are no firm rules about how best to create beautiful but accurate maps. In this case, using the latest neighbourhood deprivation data in England, mapping out the original boundaries can clearly introduce some degree of misrepresentation. There’s real value in experimenting with different cartographic techniques – and media organisations and the government would do well to think outside the box when it comes to sharing the latest findings with the public.\n\r","date":1573084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573084800,"objectID":"653da9ad6fc526bc9198fe6804c5874d","permalink":"/post/maps_conversation/","publishdate":"2019-11-07T00:00:00Z","relpermalink":"/post/maps_conversation/","section":"post","summary":"From reporting election results to issuing weather forecasts, maps offer a powerful, accessible and visually appealing way to convey complex information. Yet even the most beautiful maps can introduce some degree of misrepresentation.","tags":["visualisation","maps","deprivation","neighbourhoods","dorling","cartograms"],"title":"Even the most beautiful maps can be misleading","type":"post"},{"authors":["Samuel Langton","Reka Solymosi"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"9dcefe63ddbc3c8d5071a2871fe44c6a","permalink":"/publication/hexo_epb/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/hexo_epb/","section":"publication","summary":"Paper exploring the extent to which different methods of visualising area-based data can remedy (or exacerbate) misrepresentation by presenting results from a crowdsourced survey.","tags":["visualisation","maps","brexit","cartograms","hexograms"],"title":"Cartograms, hexograms and regular grids. Minimising misrepresentation in spatial data visualisations","type":"publication"},{"authors":null,"categories":null,"content":"\rOn a day-to-day basis, the exposure citizens have to the police is often fleeting, with officers passing by in a blur as they respond to emergency calls. Official crime figures can be disputed, but the long-term trend appears to be that levels of crime in England and Wales are heading downward. Yet it is a myth to think that an apparent drop in crime relieves some of the pressures placed on police forces.\nIn reality, the demands placed on police forces do not originate primarily from obvious criminal behaviour. Forces are increasingly burdened with what’s called “latent” crime demand, such as child sexual exploitation or modern slavery, where victims are afraid or unwilling to approach police, as well as non-crime issues, such as missing persons.\nThe volume of this demand, and the time spent on addressing it, is much more problematic to estimate, and yet the safety of the public depends on both the police and the government having a handle on such estimations.\nOut on the streets, police forces need to demonstrate an understanding of current and future demand to allocate resources efficiently and ensure calls are responded to in time. In Westminster, the funding allocations to the 43 forces of England and Wales are dictated by estimations of demand. In an era of austerity, measuring this demand has never been more crucial.\nMeasuring demand\rReports by the College of Policing and HM Inspectorate of Constabulary (HMIC), have shown how the realities of modern policing render traditional estimates of demand outdated and inaccurate.\nIn its annual review of policing in England and Wales, HMIC rates how well each police force estimates the current and future demand for their services. For their latest report, published in March 2017 and covering inspections between February 2016 and March 2017, the message is clear: most forces are doing a good job, but there is room for improvement.\nThe map below, generated using open-source spatial data, plots the latest force ratings. It paints a positive picture for England and Wales, but nine out of 43 police forces require improvement, and only a handful managed to achieve the top grade.\nA key component of improving police awareness of their demand will be refining the understanding of “latent” and non-crime demand. Latent demand often involves vulnerable people who are unwilling to come forward due to the nature of the crime, or live in communities with a widespread distrust of the police. It is therefore extremely difficult to make reliable estimations about this kind of demand on police time.\nA substantial proportion of the demands placed on police forces originate from non-crime issues such as mental health. Although estimates vary, some forces estimate that 70% to 80% of their demand is comprised of non-crime incidents. A number of forces believe that the increase in non-criminal incidents being dealt with by the police may be partially driven by cuts to other areas of the public sector.\n\rMoney depends on it\rTo ensure that police forces are allocated money appropriately, in the interests of public safety and economic efficiency, the government also needs to demonstrate an understanding of demand. The issue of police funding remains a highly charged one, and budget cuts were keenly debated during the 2017 election campaign.\nThe map here uses recent police funding figures from 2017-2018 to demonstrate the vast sums of public money involved, and how much these sums can vary between police force areas\nDespite the implications and public concern over police funding, refining the formula used to apportion money has been fraught with difficulties. A December 2015 report by MPs on the Home Affairs Committee demonstrated the problems the government has faced when attempting to estimate demand for the purposes of funding allocations. It detailed how a freshly proposed police funding formula, which aimed to simplify the estimation process, was almost immediately put on hold following the realisation that incorrect data was being used for the calculation. Although a new government consultation on a revised funding calculation was launched in September 2016, its outcome has not yet been announced.\n\rThe role of experts\rAcademic researchers are often justifiably scolded for not engaging with real-world discussions, and failing to offer practical solutions to contemporary problems in society. Yet here they clearly have something to offer.\nHMIC reported that some police forces have made significant improvements to their demand estimations by working with academic research institutions who can provide much needed methodological expertise. But such collaborations are not widespread. Only recently has there been an acceptance that a concerted collaborative effort, including academic experts, is required to improve the government’s demand-based funding formula. Hopefully in this way past errors by the Home Office can be rectified, and trust in the system restored.\nRecent history tells us that collaborations can be fruitful, and that there is the potential to drastically improve estimations of demand through such partnership. Academics, police forces and the government must bear the responsibility of instigating further change. The safety of the public may depend on it.\n\r","date":1499299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499299200,"objectID":"ae9ce2b981063f92af22ee8219c62696","permalink":"/post/crimedrop_conversation/","publishdate":"2017-07-06T00:00:00Z","relpermalink":"/post/crimedrop_conversation/","section":"post","summary":"On a day-to-day basis, the exposure citizens have to the police is often fleeting, with officers passing by in a blur as they respond to emergency calls. Official crime figures can be disputed, but the long-term trend appears to be that levels of crime in England and Wales are heading downward.","tags":["policing","crime","HMIC","funding"],"title":"When crime drops, the demands on the police don’t necessarily fall","type":"post"},{"authors":["Samuel Langton","Wouter Steenbeek"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"50f3051447846f23589fcad3f2b42874","permalink":"/publication/burg_ag/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/burg_ag/","section":"publication","summary":"Paper using Google Street View data to investigate the extent to which the physical attributes of residential homes contribute to the risk of burglary. Please feel free to contact me for a copy if you do not have access!","tags":["burglary","google street view","crime","rational choice","situational crime prevention"],"title":"Residential burglary target selection. An analysis at the property-level using Google Street View","type":"publication"}]