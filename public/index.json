[{"authors":["admin"],"categories":null,"content":"I am a former postdoc researcher and current open science enthusiast. I\u0026rsquo;ve recently finished a project on promoting and developing open science practices and computational reproducibility at Amsterdam University Medical Center.\nIf you\u0026rsquo;d like to speak about any of the above, or collaborate on an open data project, please get in touch!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/samuel-langton/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/samuel-langton/","section":"authors","summary":"I am a former postdoc researcher and current open science enthusiast. I\u0026rsquo;ve recently finished a project on promoting and developing open science practices and computational reproducibility at Amsterdam University Medical Center.","tags":null,"title":"Samuel Langton","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e87efdfe209d90ea3c6332a7cbd9d08b","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"\r\rSome time last year, I rage quit Strava. I’d had enough of the terrible data visualisations, the distraction of comparing split times, and the pestering over upgrading to a paid subscription. All I really cared about was how far I had ran, for how long, and where I had been. Since then, I first moved to an open source alternative called FitoTrack (which I would recommend as a simple alternative). I’ve now settled with recording time for each run using my Casio, and then swiftly moving on with my life.\nBut, before quitting Strava, I downloaded all my archived data. As much as I’d like to rise above obsessions over split times and average pace, I do like to look back on specific runs and check PBs periodically. When you archive Strava data, you get given your activities as individual GPX files. These files are of course utterly useless to most people, which I suppose deters many users from deleting the app. But with a fairly simple R script, you can get decent summaries of all your activities and mess around with data visualisation to your heart’s content. This is what I’ve begun to do myself, making all my data and code openly available.\nThe main challenge of the whole project was simply extracting the relevant information from the GPX files and compiling that information into a usable data frame. Printing the raw text contents of the files looks like absolute garbage, and without much experience of XML files, a lot my explorations were trial and error. I settled on using functions from the XML package to parse the files and extract information about each activity. This included stuff like the name (e.g., “Morning run”) and type (e.g., running, cycling), but importantly, also ping-level information collected throughout the duration of the activity, like the time of day, coordinate location and elevation at one or two second intervals.\nThe preliminary stuff at the top of script just loads in the packages required, lists all my GPX files in the data folder, and then imports them into RStudio using htmlTreeParse(). If you clone/download the repository and throw your GPX files into the data folder, you should be able to replicate everything shown here for your own data.\n# Load libraries.\rlibrary(pbapply)\rlibrary(XML)\rlibrary(dplyr)\rlibrary(tidyr)\rlibrary(lubridate)\rlibrary(ggplot2)\rlibrary(leaflet)\rlibrary(maptiles)\rlibrary(tidyterra)\rlibrary(sf)\r# Settings.\rtheme_set(theme_minimal())\r# Create list of all the gpx files that we have.\rfile_names \u0026lt;- paste0(\r\u0026quot;data/\u0026quot;,\rlist.files(\u0026quot;data\u0026quot;, pattern = glob2rx(\u0026quot;*.gpx\u0026quot;))\r)\r# Read them all into a list.\rraw_list \u0026lt;- pblapply(file_names, function(x){\rhtmlTreeParse(file = x, useInternalNodes = TRUE)\r}\r)\rWe can then execute the main parsing function: extracting the nodes we need and then sticking them together into a list of usable data frames. The final step uses st_as_sf() to convert the coordinates into an sf object, so we can easily calculate distances and create maps later on. I had around 200 activities and this took 1-2 monutes to run on a standard laptop.\n# Function for extracting the relevant information.\racts_clean \u0026lt;- list()\rfor (i in seq_along(raw_list)){\r# Extract name.\rname \u0026lt;- xpathSApply(doc = raw_list[[i]], path = \u0026quot;//trk/name\u0026quot;, fun = xmlValue)\r# Extract type.\rtype \u0026lt;- xpathSApply(doc = raw_list[[i]], path = \u0026quot;//trk/type\u0026quot;, fun = xmlValue)\r# Extract coords.\rcoords \u0026lt;- xpathSApply(doc = raw_list[[i]], path = \u0026quot;//trkpt\u0026quot;, fun = xmlAttrs)\r# Extract elevation.\relevation \u0026lt;- xpathSApply(doc = raw_list[[i]], path = \u0026quot;//trkpt/ele\u0026quot;, fun = xmlValue)\r# Extract time.\rtime \u0026lt;- xpathSApply(doc = raw_list[[i]], path = \u0026quot;//trkpt/time\u0026quot;, fun = xmlValue)\r# Extract information into a dataframe.\rgpx_sf \u0026lt;- data.frame(\ract_name = name,\ract_type = type,\rtimestamps = time,\rlat = coords[\u0026quot;lat\u0026quot;, ],\rlon = coords[\u0026quot;lon\u0026quot;, ],\rele = as.numeric(elevation)\r) %\u0026gt;% mutate(timestamps = ymd_hms(timestamps),\rweek_lub = week(timestamps),\ryear_lub = year(timestamps)) %\u0026gt;% st_as_sf(coords = c(x = \u0026quot;lon\u0026quot;, y = \u0026quot;lat\u0026quot;), crs = 4326) # Insert each into the list.\racts_clean[[i]] \u0026lt;- gpx_sf\r}\rEach element of the acts_clean list is a single activity, with each row representing a single GPS ping recording the time and elevation.\nView(acts_clean[[1]])\rWe can easily bind all the data frames together. At this point, I subset my activities for running-only. You can of course keep all your different activity types, but remember that later on, you will need to group_by(act_type) to get equivalent summaries, or use some equivalent loop or facet.\n# Bind together for broad summaries, then filter for runs only.\racts_sf \u0026lt;- bind_rows(acts_clean, .id = \u0026quot;act_id\u0026quot;) %\u0026gt;% filter(act_type == \u0026quot;running\u0026quot;)\rThe final key data handling step before we can begin summarising activities is spatial. At the moment, the ping-level data consists of coordinate locations recorded at one or two second intervals throughout the activity. But for mapping visuals and to easily calculate distances, we need to convert these series of points to lines. I do this by computing a union on each activity and casting the output to a linestring.\n# Convert coords to lines.\racts_lines_sf \u0026lt;- acts_sf %\u0026gt;% group_by(act_id) %\u0026gt;% summarize(do_union=FALSE) %\u0026gt;% st_cast(\u0026quot;LINESTRING\u0026quot;) %\u0026gt;% ungroup() \rWe can then calculate the distance of each line using st_length(). I made this a standalone dataframe, with no spatial information, so it can be quickly joined back later on.\n# Create df of the distances.\racts_dist_df \u0026lt;- acts_lines_sf %\u0026gt;% mutate(total_km = round(as.numeric(st_length(.)/1000), 2)) %\u0026gt;% as_tibble() %\u0026gt;% select(-geometry) \rOkay, now we can actually calculate something… Here, we create usable ping-level information for each activity, including joining back the distance data.\n# Ping-level data for every activity. pings_df \u0026lt;- acts_sf %\u0026gt;% as_tibble() %\u0026gt;% group_by(act_id) %\u0026gt;% mutate(\ract_time = max(timestamps)-min(timestamps),\ract_mins = as.numeric(act_time, units = \u0026quot;mins\u0026quot;),\rele_gain = sum(diff(ele)[diff(ele) \u0026gt; 0])\r) %\u0026gt;% ungroup() %\u0026gt;% left_join(acts_dist_df) %\u0026gt;%\rmutate(av_km_time = act_mins/total_km,\ract_id = as.numeric(act_id),\rping_id = 1:nrow(.))\r\rThis allows us to make a usable descriptive summary table in one go.\n# Summary table example.\rsum_table_df \u0026lt;- pings_df %\u0026gt;% mutate(av_km_time = round(av_km_time, 2),\ract_mins = round(act_mins, 2),\rele_gain = round(ele_gain, 0),\ract_date = format(date(timestamps), \u0026quot;%d.%m.%y\u0026quot;)) %\u0026gt;% select(act_id, act_date, act_name, act_mins, total_km, ele_gain, av_km_time) %\u0026gt;% distinct(act_id, .keep_all = TRUE) %\u0026gt;% arrange(act_id) \rThis table pretty much contains most information I would ever want from my archive data. The script should be adaptable with your own data to add or remove anything.\n\r\r\ract_id\r\ract_date\r\ract_name\r\ract_mins\r\rtotal_km\r\rele_gain\r\rav_km_time\r\r\r\r\r\r22\r\r16.02.21\r\rStretford - Jackson’s Boat - Home\r\r31.62\r\r5.72\r\r16\r\r5.53\r\r\r\r23\r\r19.02.21\r\rWater Park - Jackson’s Boat - Home\r\r33.83\r\r6.09\r\r16\r\r5.56\r\r\r\r25\r\r05.03.21\r\rBlair Witch Vibes\r\r28.70\r\r5.33\r\r18\r\r5.38\r\r\r\r26\r\r10.03.21\r\rStretford meander\r\r30.28\r\r5.72\r\r21\r\r5.29\r\r\r\r27\r\r13.03.21\r\rMisjudged bridge situation\r\r35.18\r\r6.74\r\r25\r\r5.22\r\r\r\r29\r\r20.03.21\r\rCharlie Don’t Surf\r\r34.23\r\r6.21\r\r17\r\r5.51\r\r\r\r32\r\r24.03.21\r\rFinal run\r\r55.65\r\r10.04\r\r25\r\r5.54\r\r\r\r33\r\r30.03.21\r\rJog back from test centre\r\r18.28\r\r3.59\r\r11\r\r5.09\r\r\r\r34\r\r31.03.21\r\rMisjudged bridge situation 2\r\r81.28\r\r11.58\r\r9\r\r7.02\r\r\r\r35\r\r04.04.21\r\rBlimey\r\r38.70\r\r7.81\r\r23\r\r4.96\r\r\r\r36\r\r10.04.21\r\rMorning run\r\r21.53\r\r4.37\r\r10\r\r4.93\r\r\r\r37\r\r18.04.21\r\rFirst city run\r\r29.10\r\r5.67\r\r10\r\r5.13\r\r\r\r38\r\r22.04.21\r\rVondel run\r\r25.65\r\r5.10\r\r9\r\r5.03\r\r\r\r39\r\r29.04.21\r\rRembrandt run\r\r33.52\r\r6.85\r\r24\r\r4.89\r\r\r\r40\r\r11.05.21\r\rRembrandt run\r\r23.25\r\r4.82\r\r20\r\r4.82\r\r\r\r66\r\r03.09.21\r\rTide’s out legs out\r\r37.65\r\r6.66\r\r12\r\r5.65\r\r\r\r67\r\r11.09.21\r\rHilbre\r\r43.98\r\r7.16\r\r37\r\r6.14\r\r\r\r68\r\r19.09.21\r\rRembrandt run\r\r26.12\r\r5.51\r\r18\r\r4.74\r\r\r\r69\r\r25.09.21\r\rRembrandt run\r\r39.10\r\r7.51\r\r26\r\r5.21\r\r\r\r72\r\r30.09.21\r\rRoman\r\r16.53\r\r3.58\r\r7\r\r4.62\r\r\r\r74\r\r03.10.21\r\rPT\r\r20.52\r\r3.43\r\r12\r\r5.98\r\r\r\r75\r\r04.10.21\r\rNight 5\r\r24.23\r\r5.05\r\r12\r\r4.80\r\r\r\r76\r\r08.10.21\r\rFollowing the lights\r\r24.43\r\r5.15\r\r16\r\r4.74\r\r\r\r77\r\r09.10.21\r\rPT\r\r25.25\r\r4.17\r\r13\r\r6.06\r\r\r\r78\r\r13.10.21\r\rNight 5\r\r29.10\r\r5.72\r\r20\r\r5.09\r\r\r\r79\r\r16.10.21\r\rRembrandt run\r\r27.18\r\r5.98\r\r20\r\r4.55\r\r\r\r80\r\r18.10.21\r\rPT\r\r32.12\r\r5.07\r\r19\r\r6.33\r\r\r\r81\r\r21.10.21\r\rEvening 10\r\r52.63\r\r10.10\r\r36\r\r5.21\r\r\r\r83\r\r25.10.21\r\rPT\r\r30.67\r\r5.06\r\r19\r\r6.06\r\r\r\r84\r\r27.10.21\r\rPT\r\r31.10\r\r5.11\r\r14\r\r6.09\r\r\r\r85\r\r29.10.21\r\rSloterplas 10\r\r51.30\r\r10.25\r\r22\r\r5.00\r\r\r\r86\r\r01.11.21\r\rRembrandt run\r\r41.12\r\r8.01\r\r24\r\r5.13\r\r\r\r87\r\r04.11.21\r\rPT\r\r28.37\r\r5.06\r\r17\r\r5.61\r\r\r\r88\r\r06.11.21\r\rPorridge\r\r87.33\r\r16.26\r\r32\r\r5.37\r\r\r\r89\r\r10.11.21\r\rRembrandt recovery\r\r23.27\r\r4.38\r\r14\r\r5.31\r\r\r\r90\r\r17.11.21\r\rRembrandt run\r\r26.73\r\r5.43\r\r19\r\r4.92\r\r\r\r93\r\r30.11.21\r\rKnee test\r\r30.93\r\r5.94\r\r17\r\r5.21\r\r\r\r94\r\r09.12.21\r\rKneed knees\r\r25.55\r\r5.06\r\r16\r\r5.05\r\r\r\r95\r\r16.12.21\r\rDune run\r\r30.78\r\r5.52\r\r10\r\r5.58\r\r\r\r96\r\r24.12.21\r\rExplore\r\r29.60\r\r3.92\r\r108\r\r7.55\r\r\r\r97\r\r26.12.21\r\rHill climber\r\r28.97\r\r5.09\r\r156\r\r5.69\r\r\r\r98\r\r28.12.21\r\rExplore turbo\r\r22.70\r\r3.94\r\r111\r\r5.76\r\r\r\r99\r\r02.01.22\r\rPT\r\r20.13\r\r3.46\r\r12\r\r5.82\r\r\r\r100\r\r05.01.22\r\rRembrandt run\r\r26.92\r\r5.21\r\r18\r\r5.17\r\r\r\r101\r\r09.01.22\r\rCombo\r\r47.17\r\r8.95\r\r21\r\r5.27\r\r\r\r102\r\r16.01.22\r\rSloterplas 10\r\r53.55\r\r10.19\r\r23\r\r5.26\r\r\r\r103\r\r20.01.22\r\rNight run\r\r27.38\r\r5.12\r\r16\r\r5.35\r\r\r\r104\r\r22.01.22\r\rRembrandt run\r\r24.73\r\r5.25\r\r19\r\r4.71\r\r\r\r105\r\r26.01.22\r\rEvening run\r\r26.87\r\r5.25\r\r18\r\r5.12\r\r\r\r106\r\r29.01.22\r\rSloterplas 10\r\r50.10\r\r10.16\r\r32\r\r4.93\r\r\r\r107\r\r03.02.22\r\rEvening run\r\r25.07\r\r5.11\r\r15\r\r4.91\r\r\r\r108\r\r06.02.22\r\rRembrandt run\r\r24.77\r\r5.04\r\r19\r\r4.91\r\r\r\r109\r\r08.02.22\r\rWest run\r\r40.47\r\r8.08\r\r27\r\r5.01\r\r\r\r110\r\r12.02.22\r\rWest 10\r\r53.22\r\r10.48\r\r29\r\r5.08\r\r\r\r111\r\r15.02.22\r\rRembrandt run\r\r22.57\r\r5.03\r\r18\r\r4.49\r\r\r\r112\r\r23.02.22\r\rLunchtime run\r\r16.00\r\r3.22\r\r11\r\r4.97\r\r\r\r113\r\r25.02.22\r\rMorning run\r\r24.75\r\r5.14\r\r21\r\r4.82\r\r\r\r115\r\r28.02.22\r\rWest run\r\r41.42\r\r8.03\r\r27\r\r5.16\r\r\r\r116\r\r01.03.22\r\rLunchtime run\r\r14.05\r\r3.00\r\r10\r\r4.68\r\r\r\r117\r\r12.03.22\r\rSpring\r\r25.32\r\r5.16\r\r19\r\r4.91\r\r\r\r119\r\r16.03.22\r\rSandy\r\r24.68\r\r5.03\r\r18\r\r4.91\r\r\r\r120\r\r19.03.22\r\rRembrandt run\r\r40.48\r\r7.78\r\r27\r\r5.20\r\r\r\r121\r\r21.03.22\r\rMorning run\r\r15.33\r\r2.85\r\r9\r\r5.38\r\r\r\r124\r\r27.03.22\r\rEvening run\r\r16.43\r\r3.02\r\r9\r\r5.44\r\r\r\r125\r\r06.04.22\r\rBack to it\r\r33.18\r\r6.40\r\r19\r\r5.18\r\r\r\r126\r\r15.04.22\r\rAfternoon run\r\r12.15\r\r2.27\r\r7\r\r5.35\r\r\r\r128\r\r18.04.22\r\rSloterplas 10\r\r51.45\r\r10.19\r\r27\r\r5.05\r\r\r\r129\r\r20.04.22\r\rCake\r\r36.10\r\r7.09\r\r20\r\r5.09\r\r\r\r130\r\r27.04.22\r\rRembrandt run\r\r25.22\r\r5.02\r\r18\r\r5.02\r\r\r\r134\r\r08.05.22\r\rSunday run\r\r20.52\r\r4.02\r\r14\r\r5.10\r\r\r\r135\r\r09.05.22\r\rMorning run\r\r12.73\r\r2.55\r\r9\r\r4.99\r\r\r\r136\r\r14.05.22\r\rWest meander\r\r29.15\r\r5.80\r\r16\r\r5.03\r\r\r\r138\r\r16.05.22\r\rPT\r\r13.20\r\r2.36\r\r7\r\r5.59\r\r\r\r139\r\r20.05.22\r\rRun\r\r12.77\r\r2.63\r\r9\r\r4.85\r\r\r\r140\r\r21.05.22\r\rSloterplas 10\r\r53.88\r\r10.08\r\r23\r\r5.35\r\r\r\r141\r\r25.05.22\r\rRembrandt run\r\r27.37\r\r5.01\r\r18\r\r5.46\r\r\r\r143\r\r29.05.22\r\rRembrandt run\r\r25.25\r\r5.25\r\r18\r\r4.81\r\r\r\r144\r\r04.06.22\r\rWest\r\r35.17\r\r7.12\r\r17\r\r4.94\r\r\r\r145\r\r12.06.22\r\rRembrandt run\r\r21.60\r\r4.51\r\r15\r\r4.79\r\r\r\r147\r\r10.07.22\r\rUrban trail series\r\r36.95\r\r6.45\r\r23\r\r5.73\r\r\r\r148\r\r27.07.22\r\rHeal the heel\r\r17.55\r\r3.40\r\r11\r\r5.16\r\r\r\r150\r\r01.08.22\r\rRembrandt run\r\r24.92\r\r4.34\r\r14\r\r5.74\r\r\r\r151\r\r05.08.22\r\rAlright then\r\r24.47\r\r5.04\r\r16\r\r4.85\r\r\r\r152\r\r10.08.22\r\rToasty\r\r30.18\r\r6.03\r\r16\r\r5.01\r\r\r\r153\r\r14.08.22\r\rWest\r\r37.05\r\r7.22\r\r20\r\r5.13\r\r\r\r154\r\r19.08.22\r\rWestish\r\r25.50\r\r5.44\r\r14\r\r4.69\r\r\r\r155\r\r22.08.22\r\rBlimey\r\r26.57\r\r5.13\r\r129\r\r5.18\r\r\r\r156\r\r26.08.22\r\rJogaroo\r\r15.70\r\r3.47\r\r13\r\r4.52\r\r\r\r157\r\r01.09.22\r\rWest 10\r\r48.28\r\r10.12\r\r27\r\r4.77\r\r\r\r158\r\r11.09.22\r\rPancakes\r\r63.33\r\r12.86\r\r32\r\r4.92\r\r\r\r159\r\r18.09.22\r\rSummit attempt\r\r26.00\r\r4.16\r\r116\r\r6.25\r\r\r\r160\r\r25.09.22\r\rParos 5\r\r33.90\r\r5.79\r\r163\r\r5.85\r\r\r\r161\r\r29.09.22\r\rWest (pt1)\r\r8.08\r\r1.64\r\r4\r\r4.93\r\r\r\r162\r\r29.09.22\r\rWest (pt2)\r\r35.65\r\r7.37\r\r18\r\r4.84\r\r\r\r163\r\r03.10.22\r\rRembrandt run\r\r22.20\r\r4.62\r\r17\r\r4.81\r\r\r\r164\r\r06.10.22\r\rWest\r\r43.18\r\r8.22\r\r31\r\r5.25\r\r\r\r165\r\r08.10.22\r\rGinger cake\r\r85.43\r\r16.56\r\r47\r\r5.16\r\r\r\r166\r\r13.10.22\r\rRembrandt run\r\r21.02\r\r4.34\r\r15\r\r4.84\r\r\r\r167\r\r16.10.22\r\rAmsterdam half\r\r106.42\r\r21.98\r\r70\r\r4.84\r\r\r\r168\r\r30.10.22\r\rRembrandt run\r\r22.60\r\r4.46\r\r16\r\r5.07\r\r\r\r169\r\r02.11.22\r\rWest\r\r37.88\r\r7.81\r\r24\r\r4.85\r\r\r\r170\r\r05.11.22\r\rRembrandt run\r\r24.57\r\r5.11\r\r20\r\r4.81\r\r\r\r171\r\r09.11.22\r\rWest\r\r31.20\r\r7.24\r\r25\r\r4.31\r\r\r\r172\r\r11.11.22\r\rRembrandt run\r\r22.20\r\r4.48\r\r17\r\r4.96\r\r\r\r173\r\r15.11.22\r\rWest\r\r28.17\r\r6.09\r\r17\r\r4.63\r\r\r\r174\r\r15.11.22\r\rWest finish\r\r5.15\r\r1.11\r\r5\r\r4.64\r\r\r\r175\r\r19.11.22\r\rWest\r\r31.87\r\r7.06\r\r19\r\r4.51\r\r\r\r176\r\r22.11.22\r\rPT\r\r40.63\r\r7.06\r\r25\r\r5.76\r\r\r\r177\r\r24.11.22\r\rRembrandt run\r\r31.65\r\r5.88\r\r20\r\r5.38\r\r\r\r178\r\r27.11.22\r\rWest 10ish\r\r56.10\r\r11.23\r\r39\r\r5.00\r\r\r\r179\r\r03.12.22\r\rWest 10ish\r\r50.78\r\r10.92\r\r29\r\r4.65\r\r\r\r180\r\r06.12.22\r\rWest meander\r\r29.72\r\r6.18\r\r21\r\r4.81\r\r\r\r181\r\r08.12.22\r\rWest / Sloterplas\r\r38.80\r\r8.22\r\r25\r\r4.72\r\r\r\r182\r\r13.12.22\r\rCanal loop\r\r28.10\r\r5.71\r\r19\r\r4.92\r\r\r\r183\r\r17.12.22\r\rWest / Sloterplas\r\r53.48\r\r10.98\r\r32\r\r4.87\r\r\r\r184\r\r23.12.22\r\rWirral\r\r82.30\r\r15.64\r\r33\r\r5.26\r\r\r\r185\r\r28.12.22\r\rWindy bastard\r\r23.65\r\r4.67\r\r152\r\r5.06\r\r\r\r186\r\r30.12.22\r\rLPA (pt1)\r\r7.35\r\r1.33\r\r45\r\r5.53\r\r\r\r187\r\r30.12.22\r\rLPA (pt2)\r\r49.72\r\r9.77\r\r139\r\r5.09\r\r\r\r188\r\r04.01.23\r\rRembrandt run\r\r21.73\r\r4.74\r\r18\r\r4.59\r\r\r\r189\r\r08.01.23\r\rEgmond half\r\r113.02\r\r21.85\r\r89\r\r5.17\r\r\r\r190\r\r18.01.23\r\rBack to it\r\r16.02\r\r3.44\r\r11\r\r4.66\r\r\r\r191\r\r22.01.23\r\rRembrandt run\r\r28.62\r\r4.75\r\r16\r\r6.02\r\r\r\r194\r\r04.02.23\r\rRembrandt run\r\r16.45\r\r3.99\r\r15\r\r4.12\r\r\r\r195\r\r05.02.23\r\rPT\r\r40.65\r\r7.38\r\r27\r\r5.51\r\r\r\r196\r\r08.02.23\r\rRembrandt run\r\r18.65\r\r4.23\r\r13\r\r4.41\r\r\r\r197\r\r12.02.23\r\rRembrandt run\r\r12.08\r\r2.95\r\r11\r\r4.10\r\r\r\r198\r\r07.03.23\r\rRembrandt run return\r\r17.60\r\r3.92\r\r16\r\r4.49\r\r\r\r199\r\r11.03.23\r\rRembrandt run\r\r29.93\r\r6.36\r\r20\r\r4.71\r\r\r\r200\r\r16.03.23\r\rWestish\r\r25.85\r\r5.87\r\r16\r\r4.40\r\r\r\r201\r\r19.03.23\r\rUrban pickle trail series\r\r101.98\r\r14.03\r\r41\r\r7.27\r\r\r\r202\r\r23.03.23\r\rBats\r\r14.32\r\r2.38\r\r8\r\r6.02\r\r\r\r203\r\r26.03.23\r\rZandvoort\r\r72.52\r\r12.35\r\r102\r\r5.87\r\r\r\r204\r\r24.07.23\r\rChart\r\r18.23\r\r3.16\r\r38\r\r5.77\r\r\r\r205\r\r24.07.23\r\rChart 2\r\r4.78\r\r0.85\r\r12\r\r5.63\r\r\r\r207\r\r03.10.23\r\rCome on, legs\r\r15.75\r\r2.85\r\r10\r\r5.53\r\r\r\r208\r\r07.10.23\r\rMichael Ketone\r\r16.98\r\r3.02\r\r10\r\r5.62\r\r\r\r1\r\r16.10.23\r\rJacket\r\r24.25\r\r4.31\r\r15\r\r5.63\r\r\r\r2\r\r17.10.23\r\rPT\r\r14.43\r\r2.20\r\r7\r\r6.56\r\r\r\r3\r\r21.10.23\r\rBugger this\r\r15.73\r\r3.02\r\r10\r\r5.21\r\r\r\r4\r\r25.10.23\r\rThe Heel Strike’s Back\r\r20.13\r\r4.10\r\r14\r\r4.91\r\r\r\r5\r\r30.10.23\r\rRembrandt run\r\r21.63\r\r4.32\r\r15\r\r5.01\r\r\r\r6\r\r04.11.23\r\r21 bathrooms\r\r27.33\r\r4.56\r\r15\r\r5.99\r\r\r\r7\r\r25.11.23\r\rRoom for a small one\r\r10.87\r\r2.10\r\r8\r\r5.17\r\r\r\r8\r\r29.11.23\r\rLil one\r\r11.18\r\r1.77\r\r7\r\r6.32\r\r\r\r9\r\r24.12.23\r\rSherry\r\r15.85\r\r2.50\r\r88\r\r6.34\r\r\r\r10\r\r27.12.23\r\rAudacity\r\r30.80\r\r4.75\r\r50\r\r6.48\r\r\r\r11\r\r30.12.23\r\rBeachy\r\r10.27\r\r2.01\r\r6\r\r5.11\r\r\r\r12\r\r02.01.24\r\rWatery bastard\r\r26.77\r\r4.79\r\r11\r\r5.59\r\r\r\r13\r\r04.01.24\r\rMr. Motivator\r\r12.70\r\r2.65\r\r10\r\r4.79\r\r\r\r14\r\r06.01.24\r\rJumbo\r\r15.60\r\r2.44\r\r9\r\r6.39\r\r\r\r15\r\r10.01.24\r\rNippy\r\r12.52\r\r2.61\r\r9\r\r4.80\r\r\r\r16\r\r14.01.24\r\rNo Vondelling\r\r25.70\r\r5.13\r\r15\r\r5.01\r\r\r\r17\r\r18.01.24\r\rCheese\r\r25.58\r\r4.38\r\r17\r\r5.84\r\r\r\r18\r\r22.01.24\r\rKattenlaan\r\r21.25\r\r4.23\r\r12\r\r5.02\r\r\r\r19\r\r24.01.24\r\rVondel thingy\r\r24.72\r\r5.08\r\r14\r\r4.87\r\r\r\r20\r\r27.01.24\r\rStrava are bastards. Moving to FitoTrack\r\r15.28\r\r2.76\r\r9\r\r5.54\r\r\r\r\r\rThat said, the table is quite boring. The fun of going through GPX file hell is in the visualisation that follows. I haven’t spent too long on this yet, so there’s plenty more fun to be had. Before getting into the ggplot2 chunks, I tidy up the summary table by renaming some columns and pivoting everything to long format. The pivot makes visualisation way easier later on.\n# Initial handling before visuals.\rsum_visuals_df \u0026lt;- sum_table_df %\u0026gt;% select(-act_date, -act_name) %\u0026gt;% rename(`Time (mins)` = act_mins,\r`Distance (km)` = total_km,\r`Elevation gain (metres)` = ele_gain,\r`Km pace (mins)` = av_km_time) %\u0026gt;% pivot_longer(cols = -act_id,\rnames_to = \u0026quot;measure\u0026quot;,\rvalues_to = \u0026quot;value\u0026quot;)\rNow we can easily create some visual summaries of distributions across different metrics.\n# Histograms.\rggplot(data = sum_visuals_df) +\rgeom_histogram(mapping = aes(x = value), bins = 20, fill = \u0026quot;#fc4c02\u0026quot;) +\rfacet_wrap(~measure, scales = \u0026quot;free\u0026quot;, ncol = 4) +\rlabs(y = NULL, x = NULL) +\rtheme(\raxis.text.y = element_blank()\r)\rOr plot the individual points.\n# Scatter plot of individual runs.\rggplot(data = sum_visuals_df) +\rgeom_jitter(mapping = aes(x = value, y = 0),\rcolour = \u0026quot;#fc4c02\u0026quot;, alpha = 0.5) +\rfacet_wrap(~measure, scales = \u0026quot;free\u0026quot;, nrow = 4) +\rlabs(y = NULL, x = NULL) +\rtheme(\raxis.text.y = element_blank(),\rpanel.grid.major.y = element_blank()\r)\rWe can also select individual runs to visualise things like elevation. Here, I choose a run manually by name because it was a good example of a hilly one. Be careful doing this if you tend to use the same name for different runs. You can always use the activity id numeric variable instead.\n# elevation.\rpings_df %\u0026gt;% filter(act_name == \u0026quot;Hill climber \u0026quot;) %\u0026gt;% # Name has to be distinct!\rggplot(data = .) +\rgeom_ribbon(mapping = aes(x = ping_id, ymin = min(ele)*0.5, ymax = ele, group = 1),\rfill = \u0026quot;#fc4c02\u0026quot;, linewidth = 1) +\rtheme_minimal() +\rtheme(\raxis.text.x = element_blank()\r) +\rlabs(y = \u0026quot;Elevation (metres)\u0026quot;, x = NULL)\rTo make some decent maps, we need to convert the point-level pings to lines. I do this for every activity in one go using group_by(), followed by a spatial union and then ensuring the output is treated as a line.\n# Single activity map.\r# First, create the linestrings from the points.\racts_line_sf \u0026lt;- acts_sf %\u0026gt;% group_by(act_id) %\u0026gt;% summarize(do_union=FALSE) %\u0026gt;% st_cast(\u0026quot;LINESTRING\u0026quot;) %\u0026gt;% ungroup()\rWhile not necessary, to give our activity maps some geographic context, I obtain CARTO base maps for each activity. You could do this for every activity in one go, but for now I just do it for a single example.\n# Single activity selection for examples.\ract_i \u0026lt;- 1\r# Second, obtain the osm layer for a single activity.\rosm_posit \u0026lt;- get_tiles(\rfilter(acts_line_sf, act_id == act_i),\rprovider = \u0026quot;CartoDB.Positron\u0026quot;,\rcrop = FALSE, zoom = 15\r)\rThen we can get mapping. I do a small wrangle beforehand because at some point I might add activity statistics to each individual map.\n# First we subset to get the label and keep the other data.\ract1_sf \u0026lt;- acts_line_sf %\u0026gt;% filter(act_id == act_i) %\u0026gt;% mutate(act_id = as.numeric(act_id)) %\u0026gt;% left_join(sum_table_df, by = \u0026quot;act_id\u0026quot;) # get info back.\r# Map it out.\rggplot(data = act1_sf) +\rgeom_spatraster_rgb(data = osm_posit) +\rgeom_sf(colour = \u0026quot;#fc4c02\u0026quot;, linewidth = 1) +\rtheme_void()\rWe can also, with a little effort, create an interactive map to better replicate the behaviour of the Strava app maps.\n# Interactive map for single activity.\rsingle_leaf \u0026lt;- leaflet() %\u0026gt;%\raddProviderTiles(providers$CartoDB.Positron , group = \u0026quot;Positron (default)\u0026quot;) %\u0026gt;%\raddProviderTiles(providers$OpenStreetMap , group = \u0026quot;Open Street Map\u0026quot;) %\u0026gt;%\raddProviderTiles(providers$Esri.WorldImagery, group = \u0026quot;World Imagery (satellite)\u0026quot;) %\u0026gt;% addPolylines(data = acts_line_sf %\u0026gt;% filter(act_id == act_i),\rcolor = \u0026quot;#fc4c02\u0026quot;) %\u0026gt;% addLayersControl(\rbaseGroups = c(\r\u0026quot;Positron (default)\u0026quot;,\r\u0026quot;Open Street Map\u0026quot;,\r\u0026quot;World Imagery (satellite)\u0026quot;\r))\r\r","date":1749859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1749859200,"objectID":"71798328b1fb4fe3ec81a2087180f53a","permalink":"/post/strava/","publishdate":"2025-06-14T00:00:00Z","relpermalink":"/post/strava/","section":"post","summary":"Some time last year, I rage quit Strava. I’d had enough of the terrible data visualisations, the distraction of comparing split times, and the pestering over upgrading to a paid subscription.","tags":["strava","GPX","r","spatial"],"title":"Handling and visualising archive data from Strava","type":"post"},{"authors":null,"categories":null,"content":"I have designed and taught a number of courses using R and RStudio, including the development of teaching material both on my own and with colleagues.\nAmsterdam University Medical Center I founded the first organisation-wide R user group. You can read more about our activities on the dedicated website. I also designed and taught a pilot workshop on GitHub Desktop. The material is available on GitHub. I gave a number of presentations about how to improve reproducibility in R, which are also openly available.\nNSCR Co-organise an (ongoing) series of R training workshops at the NSCR. This includes leading a two-day Data Carpenties workshop, the material for which is available on GitHub. I am also co-chairing a reading group for Regression and Other Stories for which there is a corresponding open repository. You can read more about our activities on our website.\nRoyal Statistical Society conference Led an introductory session to the tidyverse at the Royal Statistical Society (RSS) 2021 conference, along with colleagues from the University of Manchester. Material is available on GitHub.\nData visualisation in R using crime data 1-day workshop on data visualisation and mapping at the Cathie Marsh Institute (University of Manchester) in collaboration with the UK Data Service. Material can be found on the UK Data Service website.\nThe course was subsequently updated and re-ran online with the UK Data Service and Methods@Manchester. All material is available on GitHub.\nR for police forces 1-day workshop on data wrangling and visualisation for West Midlands Police Force. Please get in contact for material.\nR for criminologists 1-day workshop on data wrangling, visualisation and mapping hosted at the European Society of Criminology with colleagues from the Space Place and Crime working group. Material is available on GitHub and rpubs.\nGetting Started in R - an introduction to data analysis and visualisation 4-day comprehensive introductory course at the Methods@Manchester summer school, co-taught with colleagues. Material is available as a website.\nCluster analysis in R 1-day workshop introducing cluster analysis in R at the Cathie Marsh Institute (University of Manchester), co-taught with a colleague. Please get in contact for material.\nR for social scientists I am a qualified instructor for Data and Software Carpentries, and supported colleagues in teaching the social science workshop.\nA Beginner’s Guide to Statistics for Criminology and Criminal Justice Using R Co-authored a textbook introducing R for criminology students, published by Springer. Available from Springer\u0026rsquo;s website.\n","date":1743465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743465600,"objectID":"3dcf67677fd4025bf9369cd49793cf33","permalink":"/project/r-teaching/","publishdate":"2025-04-01T00:00:00Z","relpermalink":"/project/r-teaching/","section":"project","summary":"R course(s).","tags":["R","RStudio","teaching"],"title":"R","type":"project"},{"authors":null,"categories":null,"content":"\r\rI have had a long-time fascination with the BBC website’s top ten most read table. The most trivial of stories appear to usurp critical world events, spending hours (or days) at number one. Stories also seemingly disappear or reappear under different names. Assuming the table is dictated entirely by users’ clicks, it is a window into the priorities and interests of BBC’s readership. It is also possible that there is some manipulation by editors, such as adding a weighting (e.g., newer stories do not need as many clicks to enter the top 10 compared to older stories). Either way, it’s pretty interesting.\nWithout behind-the-scenes access to what is actually going on, I began scraping the top 10 ‘most read’ table from the BBC website (thanks to Danielle Stibbe for help on HTML nodes!). The scrape ran every five minutes, 24 hours a day. The time span is patchy due to my own errors and IT trouble, but on-and-off it runs from November 2023 to February 2024. On each scrape, a script cleans and saves the top ten table as a .csv file. Later versions of the script also include the URL of the story itself. That’s a lot of information – far too much for me to know what do with. I’ve made the data completely open so that people with more time and expertise than me can make use of it for research or teaching. If you do make use of it, please let me know!\nFor now, here’s a brief demonstration of the scraped data. All of these findings can be reproduced and edited (for your own topics of interest) using the data and code on the GitHub repository. I’ve used an example time period of 10 January to 9 February, 2024. This is one month of continuous scraping (~8,500 files).\nA very obvious thing you can do is just look at the number of scrapes – and therefore, the amount of time – that stories spent in the top ten during the scraping period. To do this, I just calculate the number of scrapes within which the story was featured (in the table: n) and then multiply by five (minutes). This of course assumes that, if a story appeared in a scrape, it remained in the top ten for five minutes. Here, then, we can see the story that spent longest in the top ten (~40 hours) was about protesters throwing soup at a painting, joint with a story about some twins on TikTok. To me, this is surprisingly dynamic: things do not stick around in the top ten long.\n\r\rTable 1: Number of scrapes and time spent in top ten. Only the five longest-spending stories are shown here.\r\r\r\rarticle_title\r\rn\r\rmins\r\rhrs\r\r\r\r\r\rProtesters throw soup at Mona Lisa painting\r\r483\r\r2415\r\r40.2\r\r\r\rTwins separated and sold at birth reunited by TikTok\r\r483\r\r2415\r\r40.2\r\r\r\rSwedish alarm after defence chiefs’ war warning\r\r458\r\r2290\r\r38.2\r\r\r\rHuge ancient city found in the Amazon\r\r437\r\r2185\r\r36.4\r\r\r\rStunning shot of polar bear drifting to sleep wins award\r\r430\r\r2150\r\r35.8\r\r\r\r\rWe can do the same thing but for each ranking position (i.e., one to ten). Here, we subset only for those stories that ever reached number one. Note that this relies on a similar assumption about time in-between scrapes.\n\r\rTable 2: For stories that reached number one only. Five longest-spending stories are shown.\r\r\r\rarticle_title\r\rn\r\rmins\r\rhrs\r\r\r\r\r\rTwins separated and sold at birth reunited by TikTok\r\r165\r\r825\r\r13.8\r\r\r\rSwedish alarm after defence chiefs’ war warning\r\r158\r\r790\r\r13.2\r\r\r\rProtesters throw soup at Mona Lisa painting\r\r139\r\r695\r\r11.6\r\r\r\rNetanyahu publicly rejects US push for Palestinian state\r\r136\r\r680\r\r11.3\r\r\r\rKing Charles diagnosed with cancer\r\r135\r\r675\r\r11.2\r\r\r\r\rMy favourite thing is to visualize the sequence of story rankings during the time period for specific topics. I do this simply by using a basic string detect for keywords. To keep the visual simple, I limit the number of individual stories visualized to six. This leaves room for an ‘Unrelated’ category (i.e., not keywords) and an ‘Other’ category for the least common but still related stories (if there are any). You can amend this as wished using the script. This script is as automated as I can make it: it will update according to the time period, number of stories, and keywords used. Below, we visualize the sequences of stories involving the term ‘Taylor Swift’ during the entire month.\nOr something that we’ve already seen is popular: TikTok. Note that this is all case-insensitive.\nWe can also select specific days and use multiple keywords. In this case, we capture a handful of words relating to Ukraine-Russia on 31 January, 2024.\nObviously, there is far more that can be done with this data, but this provides a brief overview. One thing I have explored is comparing two topics over time (see script). This way, you could assess whether certain topics decline in popularity over time, at the expense of others. For lengthy scraping periods, I think the data will require much more data handling to simplify before making a meaningful visual, or more sophisticated text analysis to identify clusters of stories. Anyway, please feel free to explore the data yourself. If you have any interesting ideas, or use the data for something fun, please do get in touch!\n","date":1711843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711843200,"objectID":"a8f0d8f9feffe6bdd78c76e1957773a3","permalink":"/post/bbc_scrapes/","publishdate":"2024-03-31T00:00:00Z","relpermalink":"/post/bbc_scrapes/","section":"post","summary":"I have had a long-time fascination with the BBC website’s top ten most read table. The most trivial of stories appear to usurp critical world events, spending hours (or days) at number one.","tags":["scraping","media","r","bbc"],"title":"Scraping 'most read' stories from the BBC","type":"post"},{"authors":["Samuel Langton","Stijn Ruiter","Linda Schoonmade"],"categories":null,"content":"","date":1705449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705449600,"objectID":"39397a170137625d3e90f862295c8a7a","permalink":"/publication/scoping_crimescience/","publishdate":"2024-01-17T00:00:00Z","relpermalink":"/publication/scoping_crimescience/","section":"publication","summary":"Scoping review on the spatial patterning of emergency demand as measured through calls for service.","tags":["policing","crime","spatial","demand"],"title":"The spatial patterning of emergency demand for police services. A scoping review.","type":"publication"},{"authors":["Samuel Langton","Tim Verlaan","Stijn Ruiter"],"categories":null,"content":"","date":1702944000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702944000,"objectID":"b5586714b729b49ce81f9033f0b5d87d","permalink":"/publication/amsterdam_gms/","publishdate":"2023-12-19T00:00:00Z","relpermalink":"/publication/amsterdam_gms/","section":"publication","summary":"Paper comparing different calculations of 'dispatched deployment time' using calls for service data from Amsterdam, Netherlands.","tags":["policing","crime","demand","calls for service","deployment time"],"title":"Operationalizing deployment time in police calls for service","type":"publication"},{"authors":null,"categories":null,"content":"Introduction to QGIS Developed and taught a 1-day introductory course at the Cathie Marsh Institute (University of Manchester). Material is available on GitHub.\nA shorter introductory course has been held online for Open Data Manchester, with an accompanying video. Material is avaiable on GitHub.\n","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"74c688af6ed1c96a9abfe2811bde5a5f","permalink":"/project/qgis-teaching/","publishdate":"2023-12-01T00:00:00Z","relpermalink":"/project/qgis-teaching/","section":"project","summary":"QGIS course(s).","tags":["QGIS","mapping","GIS","visualisation","teaching"],"title":"QGIS","type":"project"},{"authors":["Tim Verlaan","Samuel Langton","Stijn Ruiter"],"categories":null,"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"bb02419c4ad30efc81d632175ddfcc1f","permalink":"/publication/gms_tvp/","publishdate":"2023-12-01T00:00:00Z","relpermalink":"/publication/gms_tvp/","section":"publication","summary":"Article (in Dutch) in Tijdschrift voor de Politie.","tags":["crime","police","demand","calls for service"],"title":"Reactieve politie-inzet op straat","type":"publication"},{"authors":["Samuel Langton","Stijn Ruiter","Aisling Connolly","Viviane Menges"],"categories":null,"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"497c2554bb046be99e9ac1e770067dfa","permalink":"/publication/bars_prereg/","publishdate":"2023-07-01T00:00:00Z","relpermalink":"/publication/bars_prereg/","section":"publication","summary":"This study examines the spatio-temporal relationship between bars and violence in two major Dutch cities using emergency calls for service data.","tags":["policing","violence","bars","calls for service","preregistration"],"title":"Preregistration. The spatio-temporal relationship between bars and violence.","type":"publication"},{"authors":null,"categories":null,"content":"\rTowards the end of 2021, I finally plucked up the courage to take a freediving course. At the time, I had never even tried Scuba diving, but a love of swimming and a childish love of trying to swim pool lengths underwater, combined with several inspiring videos, was enough for me to give it a try. Since then, my diving has been limited to swimming pools in Amsterdam and very cold, dark Dutch lakes, together with a local dive group. But this year, I finally managed to get to Dahab, Egypt, to help prepare for the depth component of my AIDA3.\n\rFigure 1: On the ascent in Dahab. Photo taken by Pete Botman. Credit also to Pete for lending me his Amsterdam-themed fins!\r\rFreediving is pretty wholesome, and the gear-free component is part of what I enjoy so much, especially compared to Scuba. But before going to Dahab, I finally succumbed to buying a freedive watch (a Suunto D4F). The watch measures and records basic information about your dive, including important safety information such as your surface interval times. The inevitable problem is that the watch is fairly clumsy when it comes to viewing historical dives. I had all this awesome information on my dives locked away behind a black and white screen and a few buttons. I found the Suunto desktop software pretty outdated and restrictive. Opening it up gave me very Windows 95 vibes (great vibes, but not what I am looking for here).\nSo, I set about exploring whether the data could be pulled off the watch and summarised using R. The ultimate aim might be to create an app in which people can drag-and-drop their own data. But, let’s not carried away. For now, if you want to replicate what I show below, you can find everything on an open GitHub repository. If you run the code on your own data, please let me know how you found it, and what I could add to improve things. This is very much an initial exploration.\nSetup\rThe first thing you’ll need to do is load in a few packages. Nothing unusual here (mainly tidyverse-based packages). The exception is XML which we need to load in the data.\n# Load packages.\rlibrary(XML)\rlibrary(dplyr)\rlibrary(tidyr)\rlibrary(stringr)\rlibrary(lubridate)\rlibrary(ggplot2)\r\rLoading and parsing\rEach dive is archived into its own XML file. This means that even just for this year, I have 102 individual XML files. This is rather annoying but easily resolved by pasting together the working directories of all of these files based on their extension (i.e., .xml). If you use the corresponding GitHub repository, and work within the project file, all of this will run nice and smoothly. Nested within paste() is the list.files() function which simply produces a character vector for all the XML files in the specified folder. You can use this approach for any file type you want.\n# List files.\rdive_files \u0026lt;- paste(\u0026quot;data/2023/\u0026quot;, list.files(\u0026quot;data/2023/\u0026quot;, pattern = glob2rx(\u0026quot;*.xml\u0026quot;), recursive=TRUE), sep = \u0026quot;\u0026quot;)\rNow that we have all the working directories stored within the dive_files list object, we can run the xmlParse() function through the whole thing. This produces a list of XML documents, which we then further parse using xmlToList(). The output dive_list is a list of lists, but bear with me: we extract the useful information in one beautiful swoop in the next step.\n# Load data.\rdive_xml_list \u0026lt;- lapply(dive_files, xmlParse)\r# Parse into a list of lists.\rdive_list \u0026lt;- lapply(dive_xml_list, xmlToList)\r\rThe pull function\rThis step is basically the only challenging part of the process. I would love someone with more experience in wrangling XML files to improve this. At the moment, I run a function which, for each dive record, creates a tibble containing the bits of information that we need, such as the date of the dive, the depth of dives, duration and temperature.\nThe one thing that makes this odd, and involving yet more lists, is that the watch measures depth and temperature at specified time intervals (a sampling rate) of two-seconds. Nevertheless, we can pull these out using lapply() and then convert to a numeric vector within this tibble() function. We also do some trivial extras, like convert the depth readings to a negative number. This function is the key to extracting your dive data in a usable tidy format.\n# Function to pull out relevant information.\rpull_fun \u0026lt;- function(x){\rtibble(\rmode = x[[\u0026quot;Mode\u0026quot;]],\rdate = x[[\u0026quot;StartTime\u0026quot;]],\rdepth = as.numeric(lapply(x[[\u0026quot;DiveSamples\u0026quot;]], function(y){y$Depth})),\rtime = as.numeric(lapply(x[[\u0026quot;DiveSamples\u0026quot;]], function(y){y$Time})),\rmax_depth = as.numeric(x[[\u0026quot;MaxDepth\u0026quot;]]),\rdepth_minus = depth*-1,\rmax_depth_minus = max_depth*-1,\rduration = as.numeric(x[[\u0026quot;Duration\u0026quot;]]),\rtemp = as.numeric(lapply(x[[\u0026quot;DiveSamples\u0026quot;]], function(y)y$Temperature))\r) %\u0026gt;% mutate(date = str_replace_all(date, \u0026quot;T\u0026quot;, \u0026quot; \u0026quot;),\rdate_lub = ymd_hms(date),\rdate_lub_min = round_date(date_lub, \u0026quot;minute\u0026quot;),\rdate_day = round_date(date_lub, \u0026quot;day\u0026quot;))\r}\r\rNow for the magic: running this function through the list(s), and sticking it all together into a single tibble.\n# Run function through list.\rdive_df_list \u0026lt;- lapply(dive_list, pull_fun)\r# Bind together.\rdive_info_df \u0026lt;- bind_rows(dive_df_list, .id = \u0026quot;dive_id\u0026quot;)\rIn case you are not running this as we go along, the output looks like our ol’ familiar data frames with rows and columns. The data frame is in long format: we have two-second samples nested within each dive id. This is just the first ten rows.\n\r\r\rdive_id\r\rmode\r\rdate\r\rdepth\r\rtime\r\rmax_depth\r\rdepth_minus\r\rmax_depth_minus\r\rduration\r\rtemp\r\rdate_lub\r\rdate_lub_min\r\rdate_day\r\r\r\r\r\r1\r\r3\r\r2023-01-15 11:18:09\r\r1.58\r\r0\r\r2.01\r\r-1.58\r\r-2.01\r\r7\r\r6.999994\r\r2023-01-15 11:18:09\r\r2023-01-15 11:18:00\r\r2023-01-15\r\r\r\r1\r\r3\r\r2023-01-15 11:18:09\r\r1.92\r\r2\r\r2.01\r\r-1.92\r\r-2.01\r\r7\r\r6.999994\r\r2023-01-15 11:18:09\r\r2023-01-15 11:18:00\r\r2023-01-15\r\r\r\r1\r\r3\r\r2023-01-15 11:18:09\r\r1.70\r\r4\r\r2.01\r\r-1.70\r\r-2.01\r\r7\r\r6.999994\r\r2023-01-15 11:18:09\r\r2023-01-15 11:18:00\r\r2023-01-15\r\r\r\r1\r\r3\r\r2023-01-15 11:18:09\r\r0.92\r\r6\r\r2.01\r\r-0.92\r\r-2.01\r\r7\r\r6.999994\r\r2023-01-15 11:18:09\r\r2023-01-15 11:18:00\r\r2023-01-15\r\r\r\r2\r\r3\r\r2023-01-15 11:23:29\r\r1.22\r\r0\r\r2.57\r\r-1.22\r\r-2.57\r\r7\r\r6.999994\r\r2023-01-15 11:23:29\r\r2023-01-15 11:23:00\r\r2023-01-15\r\r\r\r2\r\r3\r\r2023-01-15 11:23:29\r\r2.27\r\r2\r\r2.57\r\r-2.27\r\r-2.57\r\r7\r\r6.999994\r\r2023-01-15 11:23:29\r\r2023-01-15 11:23:00\r\r2023-01-15\r\r\r\r2\r\r3\r\r2023-01-15 11:23:29\r\r2.54\r\r4\r\r2.57\r\r-2.54\r\r-2.57\r\r7\r\r6.999994\r\r2023-01-15 11:23:29\r\r2023-01-15 11:23:00\r\r2023-01-15\r\r\r\r2\r\r3\r\r2023-01-15 11:23:29\r\r1.31\r\r6\r\r2.57\r\r-1.31\r\r-2.57\r\r7\r\r6.999994\r\r2023-01-15 11:23:29\r\r2023-01-15 11:23:00\r\r2023-01-15\r\r\r\r3\r\r3\r\r2023-01-15 11:27:32\r\r1.90\r\r0\r\r7.32\r\r-1.90\r\r-7.32\r\r23\r\r6.999994\r\r2023-01-15 11:27:32\r\r2023-01-15 11:28:00\r\r2023-01-15\r\r\r\r3\r\r3\r\r2023-01-15 11:27:32\r\r2.80\r\r2\r\r7.32\r\r-2.80\r\r-7.32\r\r23\r\r6.999994\r\r2023-01-15 11:27:32\r\r2023-01-15 11:28:00\r\r2023-01-15\r\r\r\r\rOne thing I noticed here is that the maximum depth is often not recorded in one of the two-second samples. For example, in first dive (which was probably just a practice duck dive), the max depth is 2.01 metres but none of the four samples include this depth measurement. I can only guess that the sampling rate is more frequent, but that the watch only saves the two-second intervals and the max depth.\nAnyway, as we can already see, not all of these 102 dives were ‘proper’ dives. I know for sure that two of them are Scuba dives, and many of them will be dives for which I was a safety buddy. The Scuba dives are easily identifiable by the ‘mode’. To filter out safety dives, I have simply subsetted the data for dives which were shallower than eight metres. This is completely arbitrary: some of my safety dives will have been deeper, and some of my ‘proper dive’ attempts will have been shallower, but you can choose whatever threshold you’d like.\n# Remove SCUBA dives, and likely safety-buddy dives.\rdive_info_clean_df \u0026lt;- dive_info_df %\u0026gt;%\rfilter(mode == 3, max_depth \u0026gt; 8) %\u0026gt;% mutate(dive_id = as.numeric(dive_id))\r\rDive profile\rWith the data in tidy format, things get easier and more fun. The main graphic I wanted to create is a ‘dive profile’ visual for which we plot the depth on the Y-axis and the duration on the X-axis using ggplot2. In this plot, I colour the line according to the depth, which is measures at the two-second sampling rate.\n# Plot dives profile.\rdive_info_clean_df %\u0026gt;% ggplot(data = .) +\rgeom_line(mapping = aes(x = time, y = depth_minus, group = dive_id, colour = depth_minus),\rsize = 1, alpha = 1) +\rscale_colour_viridis_c() +\rscale_x_continuous(breaks = c(0, 15, 30, 45, 60, 75, 90, 105)) +\rtheme_bw() +\rgeom_hline(yintercept = 0, linetype = \u0026quot;dotted\u0026quot;) +\rlabs(y = \u0026quot;Depth (metres)\u0026quot;, x = \u0026quot;Time (seconds)\u0026quot;) +\rtheme(legend.position = \u0026quot;none\u0026quot;) \rMost of these dives will have been following a vertical rope for safety, as per the photo above. One downside of the dive profile graphic is that at first glance it might imply that the dives are covering distance along the X-axis. But, I think with the proper labeling it is a nice visual representation of the dives. We can easily differentiate outliers by both time (such as a hang at around 12 metres) and my two deepest dives (both are ~25 metres). Any other ideas, let me know. One thing I’d like to fix is the colouring by depth: at high resolution, we can see the segments for the sampling rate. I’d rather it was a smooth gradient colour. But, we go some way to remedy this later.\n\rChronology\rTo explore progression over time (which for this data, is only 2023), I create a chronological id variable based on the dates. The order of the dives is interesting, but not so much the date itself. Maybe there’s a nicer way of doing this, but it certainly made the plot later easier. The idea is to later plot max depth progression in chronological order.\n# Create chronological id and join back with the main data.\rdive_info_clean_df \u0026lt;- dive_info_clean_df %\u0026gt;% distinct(date_lub) %\u0026gt;% mutate(chrono_id = 1:nrow(.)) %\u0026gt;% right_join(dive_info_clean_df)\rWe can then visualise, by each dive, the max depth using an (upside down) lollipop graphic.\n# Basic chronology plot.\rdive_info_clean_df %\u0026gt;% distinct(chrono_id, max_depth_minus, .keep_all = TRUE) %\u0026gt;%\rggplot(data = .) +\rgeom_segment(mapping = aes(x = chrono_id, xend = chrono_id,\ry = 0, yend = max_depth_minus)) +\rgeom_point (mapping = aes(x = chrono_id, y = max_depth_minus)) +\rgeom_hline(yintercept = 0, linetype = \u0026quot;dotted\u0026quot;) +\rtheme_bw() +\rlabs(x = \u0026quot;Dive number (chronological)\u0026quot;, y = \u0026quot;Depth (metres)\u0026quot;) \rThis might prove useful in its own right, but it lacks the colour which perfectly (for me) represents the depth. The deeper you go, the darker it gets. I came up with a rather messy way of adding incremental depth measurements between zero metres (i.e., the surface) and the max depth (the turn on the line). I do this using the seq() function, specifying a sequence of 0.5 metres. This is nested within sapply() so that we generate the sequence for each dive record. The output is a list, so I then convert the sequence to a character, remove the c( and ) sandwiching the sequence, and pivot to long format by splitting rows by the comma. The final step is to make each value negative again.\n# Add incremental steps to the max_depths.\rdive_sequence_df \u0026lt;- dive_info_clean_df %\u0026gt;% mutate(depth_sequence = sapply(dive_info_clean_df$max_depth, function(x)seq(0, x, by = 0.5))) %\u0026gt;%\rselect(chrono_id, depth_sequence) %\u0026gt;% mutate(depth_sequence = as.character(depth_sequence),\rdepth_sequence = str_sub(depth_sequence, 3, -2)) %\u0026gt;% separate_rows(depth_sequence, sep = \u0026quot;,\u0026quot;) %\u0026gt;% mutate(depth_sequence = -1*as.numeric(trimws(depth_sequence)))\rWe can reproduce the chronology plot but colour the line according to the sequence, with a minor addition to the basic ggplot2 code chunk. You’ll notice that I create a mini data frame beforehand which contains only the chrono id and the max depth measure. This lets me add (coloured) points to the end of each line, but perhaps there’s some smarter ggplot2 code that would avoid the pre-step.\n# Identify the max depth for each chrono id.\rmax_depth_chrono_df \u0026lt;- dive_sequence_df %\u0026gt;% group_by(chrono_id) %\u0026gt;% summarise(max_depth_minus = min(depth_sequence))\r# Plot the sequence graph again.\rggplot() +\rgeom_line(data = dive_sequence_df, mapping = aes(x = chrono_id, y = depth_sequence, group = chrono_id,\rcolour = depth_sequence), size = 2) +\rgeom_point(data = max_depth_chrono_df,\rmapping = aes(x = chrono_id, y = max_depth_minus, colour = max_depth_minus), size = 3) +\rgeom_hline(yintercept = 0, linetype = \u0026quot;dotted\u0026quot;) +\rscale_colour_viridis_c() +\rtheme_bw() +\rlabs(x = \u0026quot;Dive number (chronological)\u0026quot;, y = \u0026quot;Depth (metres)\u0026quot;) +\rtheme(legend.position = \u0026quot;none\u0026quot;) \rFinally, we can summarise the distribution of max depth, along with the temperature of the water and duration of each dive. This step is pretty straightforward. The pivot_longer() means that we can facet_wrap() rather than create each plot individually.\n# Distribution handling and plot.\rdive_info_clean_df %\u0026gt;% select(dive_id, duration, max_depth, temp) %\u0026gt;% distinct() %\u0026gt;% rename(`Duration (seconds)` = duration,\r`Max. depth (metres)` = max_depth,\r`Temperature (C)` = temp) %\u0026gt;% pivot_longer(cols = -dive_id, names_to = \u0026quot;stat\u0026quot;) %\u0026gt;% ggplot(data = .) +\rgeom_density(mapping = aes(x = value), fill = \u0026quot;grey20\u0026quot;, colour = \u0026quot;grey20\u0026quot;) +\rfacet_wrap(~stat, scales = \u0026quot;free\u0026quot;) +\rtheme_bw() +\rlabs(x = NULL, y = NULL)\rWell, that’s pretty much it for now! It’s a start. The code should work pretty seamlessly for anyone with a Suunto D4F, but the drag-and-drop dashboard might be a long way off yet. This is partly because I don’t know to what extent this code will run on dive logs from different brands and model of watch. If you get it running using your own data (irrespective of the watch model) let me know!\n\r","date":1680912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680912000,"objectID":"5247153486b6fd4bbbd4ede9452f81e6","permalink":"/post/freedives/","publishdate":"2023-04-08T00:00:00Z","relpermalink":"/post/freedives/","section":"post","summary":"Towards the end of 2021, I finally plucked up the courage to take a freediving course. At the time, I had never even tried Scuba diving, but a love of swimming and a childish love of trying to swim pool lengths underwater, combined with several inspiring videos, was enough for me to give it a try.","tags":["xml","visualisation","r"],"title":"Wrangling freedive watch data in R","type":"post"},{"authors":["Samuel Langton","Stijn Ruiter","Tim Verlaan"],"categories":null,"content":"","date":1658448000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658448000,"objectID":"b07a916b56835925225734b6ab2a7907","permalink":"/publication/demand_detroit/","publishdate":"2022-07-22T00:00:00Z","relpermalink":"/publication/demand_detroit/","section":"publication","summary":"Paper describing the scale and composition of emergency police demand in Detroit, US. Replicates existing research elsewhere in the US.","tags":["policing","crime","demand","calls for service","mapping"],"title":"Describing the scale and composition of calls for police service. A replication and extension using open data","type":"publication"},{"authors":null,"categories":null,"content":"\r\rA couple of years ago, Reka Solymosi and I began a side-project on different ways of visualising spatial data. We were (well, still are) interested in how people interpret maps, and how these interpretations might differ depending the type of map being used, even when the underlying data is the same. We were recently invited by the UK’s Ministry of Defence to share our research experience in this area. As part of the presentation, we conducted a short survey of the MOD attendees to test how well people make estimates about spatial data when looking at different types of visualisation. This post provides a bit of background to the topic and reports on the findings from our survey. The maps, survey data and code to reproduce everything is openly available.\nBackground\rThanks to various pieces of clever research and fantastic books, we know that people can misinterpret data visualisations. Maps are no different. One reason why people can misinterpret maps is due to the size of the areas being visualised. Never have I seen more spatial visualisations flying around than during the COVID-19 pandemic: people were desperate to compare how different countries, regions or neighbourhoods were fairing, and maps are an accessible and beautiful way to convey this information. But, countries, regions and neighbourhoods can vary considerably in size. Take this map of England below. It was published by Public Health England and subsequently reported by the BBC in November 2020. It shows the different lockdown tiers which were set to come into place before Christmas that year.\nThe problem with maps like this is that the areas being mapped (local authorities) are not uniform in size or shape. Densely populated local authorities, such as those that comprise London, are almost invisible, while less urbanised areas, which are geographically large, dominate the visual. It’s a rare occasion where London not being the focus of attention is probably not a good thing, considering the number of people residing in these lockdown tiers. The makers of the map were probably well-intentioned: after all, they are using the real, raw boundaries of local authorities – but that doesn’t guarantee that the map will convey the underlying data clearly. In this particular case, it is reasonably likely that there isn’t much variation going on between these small, compact units – but of course, we cannot be certain by examining this map in isolation, and let’s be honest, not everyone is going to read the text accompanying a beautiful map.\n\rAlternative mapping methods\rWhile the government and media reporting of the pandemic gave us plenty of the above mapping examples, the issue is by no means uncommon, or particularly new. Just a few years earlier, maps were widely used to report on the result of the EU referendum in the UK (e.g. BBC). Largely due to the different voting behaviour between densely populated, urban London and the rest of the country, maps which visualised the result using raw local authority boundaries could be highly misleading. Geographically vast, leave-voting areas dominate the map, rendering the remain-strong London almost invisible. This also makes it more difficult for people to spot spatial patterns, such as like-for-like areas being geographically proximal to one another.\nThis is where alternative forms of map can be useful. By deliberately distorting raw boundaries, or by assigning geographic areas to uniform shapes, we can actually improve the accuracy with which people interpret the data underlying a map. Back in 2019, in a paper together with Reka, we examined whether alternative types of map could more accurately convey the clustering of EU referendum remain voters in London compared to the original local authority boundaries (a pre-print of this paper is also freely available here). We did this using a sample of ~800 Reddit users. We found that people were more likely to interpret information about the geographic patterning of the referendum result accurately when looking at two alternative types of map, balanced cartograms and hexograms, compared to the raw boundaries. But, other alternatives (hex and square grids) made people less likely to accurately interpret the same information, compared to the raw boundaries. So, not all alternatives worked, but our findings made it clear that (1) mapping raw boundaries can be an inappropriate way of conveying spatial information, and (2) alternative mapping methods can (but not always) convey geographic clustering better than raw boundaries. There were of course caveats to our study (e.g., generalisability), so we’d encourage you to read the full thing too!\n\rFigure 1: Proportion of remain voters in England at the local authority level, visualised using different mapping techniques. Source: Langton \u0026amp; Solymosi (2019).\r\r\rMOD extension\rFollowing our study, and an awesome training video that Reka made in collaboration with SAGE, we were contacted by a scientific adviser at the UK’s Ministry of Defence to share our musings and findings on this topic to MOD personnel. While we didn’t know the precise role and background of the attendees, we knew that many had a military background, and had expertise in analytics and scientific research. You can find the slides for this on Reka’s website. We took this opportunity to conduct another mini-survey using different data, with a slightly different focus, and of course, a very different sample of respondents.\n\rData\rThe broad motivation behind the mini-survey was comparable: Do people make worse estimates about the underlying data when observing the original boundaries of a map, in comparison to alternative methods (e.g. hex maps)? To study this, we took the example of neighbourhood deprivation in England. This is a salient example of the issue associated with mapping small areas. Neighbourhoods in England–defined as Lower Super Output Areas–are designed to be uniform by population size (~1500 residents). But, deprived areas are much more likely to be densely populated, and thus geographically small, compared to wealthier neighbourhoods, which are often sparsely populated and therefore geographically large. As a result, even regions containing lots of deprived neighbourhoods might not, at first glance and with limited information, appear particularly deprived.\nBased on our previous study, we boldly thought: hey, using alternative mapping techniques we can better convey the underlying data compared to the original neighbourhood boundaries. To test this, we obtained data on neighbourhood deprivation across three local authorities: Birmingham, Hartelpool and Burnley. As measured using the Index of Multiple Deprivation in 2019, these are some of the most deprived local authorities in England. For each region, we mapped out neighbourhood deprivation using three different mapping techniques: the raw boundaries, a hex grid (using the geogrid R package) and a dorling cartogram (using the cartogram package) scaled by resident population. Here’s Birmingham using these techniques. For simplification, we recoded the typical IMD score which runs from 1 (most deprived) to 10 (least deprived) to 1-5 (e.g., 1 and 2 were recorded to 1, and so on).\nNine maps (three regions, each visualised three different ways) were shown to the MOD participants one after the other. To try and mitigate against respondents using the previous answer as a guide, the same regions were never shown consecutively. For each map, respondents were asked to estimate the percentage of residents living in the most deprived neighbourhood (1 - dark blue). Of course, we knew the answer to this already, because we had the underlying data, but the respondents were asked to make this assessment with limited information. So, for each map, we would end up with a range of different estimates from respondents, which could then be compared to the true figure. The larger the degree of error between the estimates and the true number, the ‘worse’ the map was communicating the underlying data.\nHere, it’s worth noting that when we were discussing the survey findings during the MOD presentation, after it had been completed, a number of respondents questioned whether the colour of the first category was ‘dark blue’, but rather, a shade of purple. We return to this later. Aside from making me question my ability to distinguish different shades of blue-purple, it brought up another interesting discussion about visualisation and surveys more generally. We’ve shared a screenshot of the first page of the survey below.\n\rFigure 2: A screenshot from the first page of the survey. The survey was created is MS Forms.\r\r\rResults\rRespondents had around 24 hours to complete the survey. In total, we had 70 responses.1 Two respondents were dropped because they were tests. Two additional people were dropped for missing questions, and two more were dropped because they contained text answers. This gave us 64 completed surveys for analyses. You can re-create all the maps and analyses reported here using the code on the corresponding GitHub repository.\nIn the graphic below, we plot the distribution of respondents’ estimates for each region, and each map type. The dotted line represents the true value (reality). Of course, this true value is identical for each region, irrespective of the map type. A number of things emerge from this visual. Let’s start with the distribution of estimates when people were observing the original boundaries. In all three study regions, people tended to underestimate the percentage of residents living in the most deprived category. This is exactly what we would expect: the most deprived neighbourhoods are geographically small, occupying a smaller proportion of the map, so with no other information available, people underestimate.\nHex maps, on the other hand, which assign each neighbourhood polygon to a grid of hexagons, appear to improve the accuracy of people’s estimates. Generally speaking, respondents got closer to the true figure when viewing the hex map compared to the original boundaries. Particularly with Burnley and Hartlepool, there is a noticeable peak in the distribution close to the true value. This is quite expected: the hexagons, which are uniformly sized and shaped, reflect the similarly uniform residential size of neighbourhoods. In this way, they are quite similar to waffle charts but arranged to reflect the geography of the study region. That said, we know from our previous study using the EU referendum data, and a visual inspection of the maps used here, that hex grids can massively distort the spatial distribution of polygons. So, while the hex maps used here have some clear benefits when it comes to interpreting aggregate information, the spatial patterning itself might be lost along the way. Interestingly, there is no such peak in respondent estimates around the true figure for the Birmingham hex map. This might be a result of the sheer number of neighbourhoods compared to Burnley and Hartlepool, or because there was less variation in the size of neighbourhoods nested within the city\nWhen observing the dorling maps, people often slightly overestimated the percentage of people residing in the most deprived neighbourhood. It’s not immediately clear why this is the case (at least, not to me), but this overestimation was pretty common across all three regions. Even though Lower Super Output Areas are designed to be fairly similar in residential size, there is still some variability, which is reflected in the differently sized circles representing each neighbourhood. Interestingly, this was the only map type which actually conveyed data about residential size. But, without any additional information or clarification (e.g., an extra legend), the dorling maps appear to have misled respondents fairly consistently across regions.\nThe plot below visualises the same data slightly differently. Here, we plot the difference between each respondents’ estimate and the true figure, jittered slightly along the y-axis to avoid overlap between points. A boxplot summarises the overall spread. This does a reasonable job at demonstrating the degree of error in respondents’ estimates. But, it also shows the considerable spread in responses – making this estimate is not easy, and respondents clearly sometimes make completely erroneous guesses, possibly due to the survey design. In this particular case, the spread is minimised (overall) when using hex maps with a relatively small number of areas, as is the case with Hartlepool and Burnley.\n\rDiscussion\rThe findings from our mini-survey of MOD participants raise a number of interesting points – but also plenty of further questions. First, we found pretty good evidence to suggest that, in the absence of detailed information, visualising raw boundaries can misrepresent the data underlying a map. Here, respondents tended to underestimate the proportion of residents living in highly deprived neighbourhoods when observing a map of the raw boundaries. This is precisely what we would expect due to their small geographic size in comparison to wealthier neighbourhoods. Instead, by assigning neighbourhoods to a hex grid, which are uniform in size and shape, respondents were able to make fairly accurate estimates. But, this benefit comes at a cost, namely, the distortion of spatial patterning. The dorling cartogram introduced the opposite effect as the original boundaries, with people tending to slightly overestimate the overall proportion. So, alternative methods are not always ‘better’ but are certainly worth considering. Ultimately, it just depends on the aim of the research, and the message you want to convey with the visualisation.\nConducting the mini-survey itself brought up an interesting learning point on survey design for these kinds of questions. As we noted earlier, many respondents questioned the description of the ‘most deprived’ category as ‘dark blue’. We used a colourblind friendly palette (viridis), but clearly, referring to colours by name in isolation is problematic. Different people will interpret the same colours differently, even with colourblind friendly palettes like viridis. You can try this out for yourself online using the viridis documentation. Fortunately, because we referred to the category by both colour and number, I think we largely avoided total confusion, otherwise we’d expect a clear bi-modal distribution in answers, or many ‘zero’ answers. Nevertheless, for a full-scale survey with a robust design and a generalisable sample, we would want to pilot the survey thoroughly beforehand to weed-out issues like this early on.\nSpeaking of full-scale surveys: of course, mini-surveys like this are fun and interesting, and as noted, make useful learning experiences, but there is a huge ‘generalisability’ question over the results. We don’t know much about our MOD sample, so we cannot even generalise to the organisation itself. Any future work which aspires to generalisation might want to consider what populations are interesting and useful, and which map types (and for what purposes) are most relevant for that population. If anyone has pointers, suggestions or feedback, please do get in touch!\n\r\rIt’s worth noting that, for the results presented in the MOD meeting, respondents only had around 8 hours. This gave us enough time to throw the R code together before the presentation. So, for the presentation we had 45 usable responses. The results between that the those presented here are very similar.↩︎\n\r\r\r","date":1643241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643241600,"objectID":"9e942a9a2fa93c84d7b0c0b2a6b9fb44","permalink":"/post/mod_blog/","publishdate":"2022-01-27T00:00:00Z","relpermalink":"/post/mod_blog/","section":"post","summary":"A couple of years ago, Reka Solymosi and I began a side-project on different ways of visualising spatial data. We were (well, still are) interested in how people interpret maps, and how these interpretations might differ depending the type of map being used, even when the underlying data is the same.","tags":["mapping","deprivation","visualisation","R"],"title":"Misrepresentation in maps: mini-survey results from the Ministry of Defence","type":"post"},{"authors":["Samuel Langton","Jon Bannister","Mark Ellison","Muhammad Salman Haleem","Karolina Krzemieniewska-Nandwani"],"categories":null,"content":"","date":1627603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627603200,"objectID":"3e261b8875ef302cedffa90ddf4dce48","permalink":"/publication/pmih_preprint/","publishdate":"2021-07-30T00:00:00Z","relpermalink":"/publication/pmih_preprint/","section":"publication","summary":"Paper estimating the scale of police demand originating from incidents involving people with mental ill-health.","tags":["policing","demand","geography","text mining","mental health"],"title":"Policing and mental ill-health. Using big data to assess the scale and severity of, and the frontline resources committed to, mental ill-health related calls-for-service","type":"publication"},{"authors":["Samuel Langton","Anthony Dixon","Graham Farrell"],"categories":null,"content":"","date":1624579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624579200,"objectID":"687cebdc47ac465f62dd85b58b47f576","permalink":"/publication/covid_smallarea_preprint/","publishdate":"2021-06-25T00:00:00Z","relpermalink":"/publication/covid_smallarea_preprint/","section":"publication","summary":"Paper examining local varition in the COVID-19 lockdown crime drop.","tags":["crime","covid","geography","longitudinal","clustering"],"title":"Small area variation in crime effects of COVID-19 policies in England and Wales","type":"publication"},{"authors":["David Buil-Gil","Angelo Moretti","Samuel Langton"],"categories":null,"content":"","date":1616716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616716800,"objectID":"5a569bf2d1ef67b6f607c9475ff58a7e","permalink":"/publication/crimsim_preprint/","publishdate":"2021-03-26T00:00:00Z","relpermalink":"/publication/crimsim_preprint/","section":"publication","summary":"Paper using simulated data to investigate bias in police recorded crime at multiple spatial scales.","tags":["policing","crime","data bias","reporting","simulation"],"title":"The accuracy of crime statistics. Assessing the impact of police data bias on geographic crime analysis","type":"publication"},{"authors":["Samuel Langton","Anthony Dixon","Graham Farrell"],"categories":null,"content":"","date":1614816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614816000,"objectID":"f0c9fe532fae20ea5c9a95cf388ac477","permalink":"/publication/covid_crimescience/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/publication/covid_crimescience/","section":"publication","summary":"Paper examining crime in England and Wales during the first six months of the pandemic.","tags":["policing","crime","covid","reporting"],"title":"Six Months In. Pandemic Crime Trends in England and Wales","type":"publication"},{"authors":["Monsuru Adepeju","Samuel Langton","Jon Bannister"],"categories":null,"content":"","date":1612224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612224000,"objectID":"90f031f7fde7e47d9b6ab09707e4baaf","permalink":"/publication/akmedoid_jcss/","publishdate":"2021-02-02T00:00:00Z","relpermalink":"/publication/akmedoid_jcss/","section":"publication","summary":"Paper detailing a substantive demonstration of ak-medoids clustering on simulated and real police-recorded crime data.","tags":["crime","longitudinal","clustering","simulation","crime drop"],"title":"Anchored k-medoids. A novel adaptation of k-medoids further refined to measure long-term instability in the exposure to crime","type":"publication"},{"authors":null,"categories":null,"content":"\r\rIn recent years, a consensus has begun to emerge over the suitability of street segments for visualising and analysing the geographic patterning of crime. A number of studies have argued / demonstrated that these so-called ‘micro’ places are not only theoretically meaningful behavioural spaces, but that most action occurs among street segments. This makes them particularly useful units of analysis for studying criminal behaviour and designing crime-reducing interventions.\nBut how useful is the ‘street segment’ in different urban contexts? To what extent are these micro units comparable between cities, and between countries? So far, I haven’t found much data-driven research into the uniformity and ‘universality’ of street segments in terms of their physical characteristics. But to me, there are reasons to examine this issue further. In Sweden, for instance, street segments have been described as “virtually useless” because many urban areas are designed to be car-free, and consequently do not have ‘street segments’ in the same sense that other countries do.\nUnfortunately for us, the UK does not have the foresight to have many car-free urban areas. However, it does contain cities with highly variable histories of urban development. It seems / seemed implausible to me that the physical characteristics of street segments in, say, Milton Keynes, a purpose-built town which did not exist until the 1960s, were comparable to those in Edinburgh, which gained city status around 400 years ago, or Birmingham, which underwent dramatic urban regeneration following extensive bombing in World War II.\nTo satisfy my own curiosity, and to gauge people’s interest / enthusiasm for the topic, I have spent a bit of time exploring the physical characteristics (length and sinuosity) of street segments in the UK. All the code used to generate these (very preliminary) findings is openly available. This work represents early thoughts and explorations, so if anyone has any comments or pointers, please do get in touch.\nStreet segments\rStreet segments are defined as “the two block faces on both sides of the street between two intersections”. While the definition is geared towards the grid-based street networks of North America, we can apply a comparable concept to the UK using data from Ordnance Survey Open Roads. Here, individual streets are defined (and given a unique ID) based on meeting intersections. As an example, the following visual plots street segments within a 500-metre buffer in the centre of Manchester. Each road ID has been coloured to help demonstrate the individual street segments (although note that there are a limited number of colours available).\nTo me, the Ordnance Survey data fits the definition of street segments rather well. Each coloured line (i.e. individual ID) represents both sides of the street, and each line is segmented based on its intersection with another road.\nI collated this data for eight major towns and cities in the UK. Roads were clipped according to the town or city boundary (defined using the Open Street Map API – see code), and those roads classified as motorways (highways) were removed. As evidenced below, the city boundaries sometimes include satellite towns and hamlets. This raises a related discussion about the universality of street segments in terms of urban/rural areas and how city boundaries are defined – I might return to this another time.\nFor now, I focus on describing the physical characteristics of these street segments in two ways: length and sinuosity. By examining the degree to which street segments are homogeneous in length and sinuosity, both within and between study regions, we might shed some light on the degree to which they can be considered uniform, and in turn, ‘universal’ units of analysis.\nLength is fairly self-explanatory. We are talking about it in the conventional sense (metric length) rather than other definitions (e.g. topological length). Although there is not much information out there, there appears to be at least some variation in the length of street segments used in crime concentration research. It is often added as a control variable, since longer streets might have more crime by virtue of their length, rather than whatever we are interested in.\nBut would excessive ranges (maximum - minimum) and variation in street segment length undermine the idea that street segments are a universally useful micro unit of analysis? Can we confidently compare study regions which are comprised of street segments with drastically different length characteristics (e.g. mean, variation, minimum, maximum)?\nSinuosity is a fairly new concept to me. In crude terms, and at the risk of annoying geographers and mathematicians, sinuosity in this context represents a measure of ‘straightness’. It is calculated by dividing the actual length of a line by the straight line distance (i.e. the distance between the start and end point). So, a perfectly straight street segment, for which the actual length and straight line length are the same, will have a sinuosity of 1, the smallest value possible. The example below, using street segments randomly sampled from Birmingham, demonstrates this concept. The black line represents the raw street segment, and the red line is the shortest distance from start to finish.\nAgain, the reasoning behind examining sinuosity is to scrutinise the universality of street segments as meaningful behavioural spaces to study crime. A huge amount of variation in the sinuosity of street segments (either within the same study region, or between different study regions) might give reason to question this. Modern, grid-based cities might be almost entirely comprised of street segments with a sinuosity of 1. Cities with a long and convoluted history of urban development might have far more irregularity or quirks (e.g. right figure above). In which case, would the definition of ‘street segment’ generate units of analysis which still hold the same theoretical meaning?\n\rCharacteristics\rFor now, I have generated a series of descriptive statistics to try and summarise the length and sinuosity of the eight study regions. On top of statistics on raw length (e.g. ‘Mean length’) and sinuosity (e.g. ‘Mean sin.’) for each study region, I have calculated the proportion of street segments which are straight (‘Prop. straight’). This is simply the proportion of streets with a sinuosity of 1 (when rounded to six decimal places).\nThe figures on length alone generate some interesting discussion. The maximum lengths, which range from around 1200 metres to nearly 3600 metres, are clearly dragging the mean up. Recall that we removed motorways, but not A-roads or B-roads, which can be quite long. That said, A and B-roads are common even in dense urban areas, and contain plenty of opportunities for crime (e.g. petrol stations, residential dwellings). Removing them would come at a cost. The median might prove more useful. Quite a few study regions have a median street segment length of ~60 metres, but for instance, Edinburgh is noticeably longer (~72 metres) and Manchester a little shorter (~53 metres).\nThe minimum values are amusingly small – perhaps showcasing some quirk in how the Ordnance Survey generate segments (e.g. roundabouts, which in previous research required additional data handling).\n\r\r\r\rCity\rMean length (metres)\rMedian length (metres)\rSD length (metres)\rMin. length (metres)\rMax. length (metres)\rMean sin.\rMedian sin.\rSD sin.\rMin. sin.\rMax. sin.\rProp. straight\r\r\r\rBirmingham\r91.29\r69.28\r83.48\r0.01\r3573.64\r1.05\r1\r0.30\r1\r26.13\r0.60\r\rLeeds\r90.32\r62.62\r111.75\r0.01\r2716.05\r1.05\r1\r0.32\r1\r30.39\r0.62\r\rM. Keynes\r96.94\r61.80\r137.50\r0.03\r3489.49\r1.08\r1\r1.81\r1\r219.30\r0.48\r\rEdinburgh\r102.59\r72.44\r126.60\r0.11\r3116.51\r1.07\r1\r0.34\r1\r16.53\r0.57\r\rManchester\r68.02\r53.40\r59.50\r0.02\r1227.02\r1.07\r1\r4.04\r1\r632.77\r0.74\r\rLiverpool\r81.10\r60.42\r72.65\r0.03\r1557.76\r1.05\r1\r0.32\r1\r28.99\r0.68\r\rNewcastle\r85.51\r61.00\r95.72\r0.13\r3010.29\r1.07\r1\r0.46\r1\r29.08\r0.64\r\rYork\r111.76\r69.35\r159.06\r0.10\r3442.73\r1.06\r1\r0.25\r1\r8.66\r0.58\r\r\r\rHow do these UK cities compare to elsewhere? One study from The Hague (the Netherlands) reported a mean length of 94 metres and a standard deviation of 108 metres. This makes it fairly similar to Leeds, for instance. A study in Boston (United States) reported a mean of 130 metres, although no standard deviation was mentioned. The minimum length in that study was ~3 metres, and maximum ~640 metres. Another in Brooklyn Park (United States) had a mean length of 182 metres. This is considerably different to say, Manchester (Mean = 68 metres, Median = 53 metres), and strikes me as quite important when it comes to treating street segments as a universal micro unit of analysis.\nWhat about sinuosity? Generally speaking, it appears that street segments are fairly straight. The median sinuosity across all study regions is 1. Mean sinuosity is fairly low (but similar to the left example above – which isn’t exactly straight), although this may be dragged up by outliers, such as in Manchester.\nThe proportion of street segments which are straight (sinuosity = 1, to nearest six decimal places) is quite high, but varies quite considerably between cities. In Milton Keynes, only 48% of street segments are straight, whereas this figure is 74% in Manchester. If you had asked me beforehand, I would have guessed that Milton Keynes would have had the highest percentage out of all the study regions, due to its unusual (for the UK) grid system. The surprise might result from the study region boundaries, which for Milton Keynes included villages and hamlets outside of the main town centre. Note that it still has a median sinuosity of 1 due to rounding.\n\rConclusion\rThis has been a very preliminary investigation into the homogeneity of street segments in terms of length and sinuosity. The idea is / was that street segments should have reasonably homogeneous physical characteristics if we want to consider them to be ‘universal’ micro-level units of analysis in crime concentration research, and represent comparable behavioural spaces to study crime.\nTo me, findings certainly suggest that it is worth investigating this topic further. The length of street segments appears to vary quite considerably between study regions – both those presented here, and those in existing research. These differences may well narrow with further data handling, for instance, the removal of outliers or specific features such as a roundabouts. However, in that case, we might consider (1) outlining how and why street segments were kept/removed from analysis in a manner which is systematic and transparent, (2) demonstrating or discussing the implications of keeping/removing street segments, and/or (3) refining the definition of street segment to exclude those roads which are far too short or long to be reasonably comparable.\nIn terms of sinuosity, I have been surprised to see that many street segments in these UK study regions are more or less straight. However, the degree to which this holds true varies between study regions. Outliers with exceptionally high sinuosity might prove problematic in terms of keeping micro units of analysis uniform and comparable. In which case, the same three points above would apply regarding transparency, implications and definitions.\nNext steps might be:\n\rPerform some further data handling on the Ordnance Survey data to establish whether some systematic rules can create study regions with uniform street segments in terms of length and sinuosity.\rExplore the implications of the above for crime analysis.\rExplore alternative definitions of street segments (e.g. segment roads based on angle change between nodes, rather than intersections alone).\rExamine other physical characteristics (e.g. width).\rInternational comparisons.\r\r\rFurther reading\rBuil-Gil, D., Moretti, A., \u0026amp; Langton, S. (2020). The integrity of crime statistics: Assessing the impact of police data bias on crime mapping. Pre-print.\nSteenbeek, W., \u0026amp; Weisburd, D. (2016). Where the action is in crime? An examination of variability of crime across different spatial units in The Hague, 2001–2009. Journal of Quantitative Criminology, 32(3), 449-469.\nTaylor, R. B. (2015). Community criminology: Fundamentals of spatial and temporal scaling, ecological indicators, and selectivity bias (Vol. 12). NYU Press.\nWeisburd, D., Groff, E. R., \u0026amp; Yang, S. M. (2012). The criminology of place: Street segments and our understanding of the crime problem. Oxford University Press.\n\r","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609718400,"objectID":"1df6fe76d2e760b498910bf2df3fda7e","permalink":"/post/street_segments_1/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/post/street_segments_1/","section":"post","summary":"In recent years, a consensus has begun to emerge over the suitability of street segments for visualising and analysing the geographic patterning of crime. A number of studies have argued / demonstrated that these so-called ‘micro’ places are not only theoretically meaningful behavioural spaces, but that most action occurs among street segments.","tags":["scale","geography","micro","maps"],"title":"The universality of street segments: length and sinuosity","type":"post"},{"authors":["Monsuru Adepeju","Samuel Langton","Jon Bannister"],"categories":null,"content":"","date":1607126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607126400,"objectID":"2b259935ef42f9faf77e03b6a3653c91","permalink":"/publication/akmedoid/","publishdate":"2020-12-05T00:00:00Z","relpermalink":"/publication/akmedoid/","section":"publication","summary":"Paper and documentation for an R package to implement anchored k-medoids, a method for clustering longitudinal data.","tags":["crime","clustering","kmeans","longitudinal","R"],"title":"Akmedoids R package for generating directionally-homogeneous clusters of longitudinal data sets","type":"publication"},{"authors":["Samuel Langton","Reka Solymosi"],"categories":null,"content":"","date":1605830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605830400,"objectID":"0d1b58e66d793f9244e74e142fe72bf5","permalink":"/publication/osm_chapter/","publishdate":"2019-11-20T00:00:00Z","relpermalink":"/publication/osm_chapter/","section":"publication","summary":"Forthcoming book chapter on using open data to study crime and place including a practical exercise in R.","tags":["open data","maps","crime","transport","open street map"],"title":"Open Data for Crime and Place Research. A Practical Guide in R","type":"publication"},{"authors":["Graham Farrell","Samuel Langton","Anthony Dixon"],"categories":null,"content":"","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603065600,"objectID":"48472ac5e3848d8cc90b8ca51cf2ea9d","permalink":"/publication/covid_policinginsight/","publishdate":"2020-07-31T00:00:00Z","relpermalink":"/publication/covid_policinginsight/","section":"publication","summary":"Article for Policing Insight examining police-recorded crime in England and Wales between March and August.","tags":["crime","COVID","policing","visualisation"],"title":"Six months in. Pandemic crime trends in England and Wales to August 2020","type":"publication"},{"authors":["Samuel Langton","Wouter Steenbeek","Monsuru Adepeju"],"categories":null,"content":"","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"515feb16df37541678cd68c677e91c85","permalink":"/publication/spatialscale_preprint/","publishdate":"2019-09-18T00:00:00Z","relpermalink":"/publication/spatialscale_preprint/","section":"publication","summary":"Paper exploring concentration and variance in offender residences across multiple (nested) spatial scales.","tags":["spatial scale","maps","crime","offenders","neighbourhoods"],"title":"An examination of variability in offender residences across different spatial scales. A case study in Birmingham","type":"publication"},{"authors":["Samuel Langton"],"categories":null,"content":"","date":1596153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596153600,"objectID":"24791839267f4b5d4ab4af2a850c70cb","permalink":"/publication/covid_leeds1/","publishdate":"2020-07-31T00:00:00Z","relpermalink":"/publication/covid_leeds1/","section":"publication","summary":"Stats bulletin reporting end of month counts for crime and anti-social behaviour in Greater London before and after lockdown.","tags":["crime","COVID","policing","visualisation"],"title":"Crime and Anti-social Behaviour in Greater London","type":"publication"},{"authors":null,"categories":null,"content":"\r\rOver the past few weeks I have spent a bit of time exploring police recorded crime trends before and after the UK-wide lockdown.\nThere has been talk of lockdowns representing the largest criminological experiment in history. Of course, determining the impact of this ‘experiment’ using police recorded crime data is not straightforward. Lots of parameters will have changed beyond people’s routine activities, amongst them, people’s ability and willingness to report crimes to the police, and policing resource allocation. It might be some time before we can fully understand how lockdowns have curbed the spatial and temporal patterns of crime.\nIn the meantime, open police recorded crime data can offer insight into how criminal behaviour might have changed in the past few months.\nVisualising trends\rA recent article of mine explored how crime trends changed in March and April following the lockdown using London as a case study. It used a descriptive visualisation which was inspired by those used in the FT to track coronavirus deaths whilst accounting for seasonal changes in death rates. A key characteristic of long-term crime trends is seasonality. To disentangle fluctuations in crime, and try to pinpoint irregularity resulting from lockdown measures, we must account for typical monthly fluctuations. Even then, monthly data is far from ideal, but you can’t have everything (especially when working from your living room).\nWe now have some updated data – up to and including May, when some lockdown rules were relaxed – which demonstrates the start of a turnaround in crime trends. The code to make this visual is now available on GitHub.\nThe dramatic declines in burglary, robbery, theft and shoplifting observed in March and April have begun to reverse, or at least stabilise. Bicycle theft has more or less returned to normal when compared with previous years. As lockdown restrictions have eased, opportunities for crime have begun to (re)emerge. Counts are still unusually low amongst many crime types, but we are certainly witnessing a turnaround.\nGiven that we can attribute most ASB incidents to lockdown breaches, it is no surprise to see counts beginning to decline. It will be interesting to see where this goes in June and July. We might see a resurgence of ‘traditional’ ASB amidst a fall in incidents associated with lockdown breaches, which will complicate the picture somewhat. We will be able to disentangle this through the analysis of secure records which provide some context to incidents (e.g. text logs), but the continued closure of universities (and thus, many secure data facilities) might slow this down.\n\rDrug activity\rOne of the most interesting talking points is around drugs. Initially, some reports suggested that drug use would rise during lockdown, partly due to the resultant strains on mental health. Now, there are reports that usage, at least for party drugs like ecstasy and cocaine, has declined. We might then have expected a fall in drug offences, which includes possession, consumption and/or supply. Instead, we’ve seen an unprecedented increase throughout April and May.\nIt is perfectly plausible that drug activity, such as those relating to ‘County Lines’, has not changed much, or declined, even amidst an increase in the number of offences being recorded by the police. Some forces reported an increase in arrests simply due to dealers sticking out on empty streets. This has been described as like shooting fish in a barrel for police, and represents a good example of how trends in police recorded crime – however useful – should be interpreted carefully.\n\rConclusion\rThe monthly release of open police crime data in the UK offers a useful on-the-fly opportunity to examine how the lockdown may have impacted on crime. I’ll be updating this visual periodically upon new data releases to see how things are changing. Please do get in contact if you have any suggestions or comments so far.\n\rFurther reads\rAshby, M. (2020). Changes in police calls for service during the early months of the 2020 coronavirus pandemic. Pre-print.\nCampedelli, G. M., Aziani, A., \u0026amp; Favarin, S. (2020). Exploring the effect of 2019-nCoV containment policies on crime: The case of los angeles. Pre-print.\nGerell, M. (2020). Minor covid-19 association with crime in Sweden, a five week follow up. Pre-print.\nHalford, E., Dixon, A., Farrell, G., Malleson, N., \u0026amp; Tilley, N. (2020). Crime and coronavirus: social distancing, lockdown, and the mobility elasticity of crime. Crime Science, 9(1), 1-12.\nStickle, B., \u0026amp; Felson, M. (2020). Crime Rates in a Pandemic: the Largest Criminological Experiment in History. American Journal of Criminal Justice, 1-12.\n\r","date":1593993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593993600,"objectID":"bcc70011d28eb4ae6ebacc320e11edf3","permalink":"/post/update_lockdown/","publishdate":"2020-07-06T00:00:00Z","relpermalink":"/post/update_lockdown/","section":"post","summary":"Over the past few weeks I have spent a bit of time exploring police recorded crime trends before and after the UK-wide lockdown.\nThere has been talk of lockdowns representing the largest criminological experiment in history.","tags":["policing","crime","COVID","R"],"title":"Update: lockdown crime trends","type":"post"},{"authors":null,"categories":null,"content":"\r\rUnderstanding what has happened to crime during lockdown is challenging. We are in uncharted territory and it’s proving hard to draw definitive conclusions from the latest police recorded crime data. But a few trends, like a spike in antisocial behaviour and an increase in drug offences, are beginning to emerge.\nChanges in people’s movements and activities can impact on the opportunities available to potential offenders and, in turn, the volume and types of crime being committed. As a case study, we can look at Greater London and consider crimes recorded by the Metropolitan Police Service before and after the nationwide lockdown.\nUsing this data, I have visualised how the Met recorded antisocial behaviour and different crime types on a month-by-month basis. Importantly, there is enough open data to compare trends across years. This is useful for studying crime because many offence types are seasonal.\nFor instance, bicycle theft tends to increase in spring (around when the lockdown began) so looking at 2020 in isolation would be uninformative, or even misleading. This mimics the approach taken when visually tracking coronvirus fatalities due to the seasonality of death rates.\nCrimes such as burglary, shoplifting, robbery and theft all experienced remarkably sudden declines during April in comparison to previous years. Although we cannot differentiate individual days and weeks, some of these declines were already evident in March.\nThis aligns with what many will have expected. With most people spending so much time at home, shopping limited to essentials, and movement restricted to necessary travel and exercise, the opportunities available to commit such offences were severely curtailed.\nAnti-social behaviour\rOne of the starkest trends observed is antisocial behaviour. Even by the end of March, recorded incidences were unusually high. But between March and April, the number of cases skyrocketed by over 270%. An increase at this time of year is expected, but a rise of this magnitude is unprecedented.\nIt is plausible that the lockdown resulted in a genuine shift in “traditional” antisocial behaviour, such as nuisance noise from neighbours. But the National Police Chiefs’ Council has said that this increase can largely be attributed to people breaching lockdown guidelines.\nThe picture is not so clear for criminal offences. There has not yet been any discernible impact on public order or the possession of weapons. Violence and sexual offences appear to have declined more steeply than usual.\nHowever, using these charts in isolation, it is difficult to definitively attribute these changes to the lockdown. Even during less remarkable times, police recorded crime patterns can be subject to short-term blips. Indeed, visualising the long-term trend for violence and sexual offences demonstrates that similarly volatile changes have occurred before.\nDrug offences appear to have gone up in comparison to previous years. Addiction experts have speculated that the lockdown could result in an increase in drug usage among recreational users. At the same time, some police forces have suggested that the demand for drugs has remained the same, but that drug dealers now stick out on otherwise empty streets, increasing the number of arrests.\nIn other words, these changes may not necessarily reflect actual changes in criminal behaviour. The long-term picture, visualised below, demonstrates that April’s increase might also be part of a wider upward trend in Greater London. The rise in recent years might be partly attributable to action taken over county lines drug networks.\n\rExercising caution\rSome have labelled the nationwide lockdown as an ideal natural experiment to study how a shift in the opportunities available to offenders might impact on crime. But of course, it is not that simple.\nNumerous parameters have changed. Recording practices for lockdown breaches have had to be adapted overnight. The willingness and ability of people to report crimes to the police, particularly for domestic offences, will have changed in ways that cannot yet be quantified. Calls to a national domestic abuse helpline have increased, but open police records do not flag which violent and sexual offences, for instance, were domestic.\nPolicing priorities have also had to be adjusted in response to vague government guidelines and there has been considerable strain on officers as a result.\nOne way of overcoming shortcomings in police data is by using victimisation surveys such as the Crime Survey for England and Wales that can capture crimes not reported or identified by the police. This largely resolves issues around under-reporting and includes additional crime types such as cybercrime which might have increased during the lockdown. However, the survey data is not month-by-month and will not be released for some time.\nFor now, open data in the UK represents a unique opportunity to track police recorded crime – as long as it is used with a degree of caution.\n\r","date":1591920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591920000,"objectID":"c2cb3cfc0d028302741c86810fe0df76","permalink":"/post/lockdown_conversation/","publishdate":"2020-06-12T00:00:00Z","relpermalink":"/post/lockdown_conversation/","section":"post","summary":"Understanding what has happened to crime during lockdown is challenging. We are in uncharted territory and it’s proving hard to draw definitive conclusions from the latest police recorded crime data.","tags":["crime","policing","covid","lockdown","London"],"title":"Lockdown crime trends. What we know and what we don't","type":"post"},{"authors":["David Buil-Gil","Samuel Langton"],"categories":null,"content":"","date":1575676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575676800,"objectID":"a8f2f85821952fd147e4f3325dc85fa1","permalink":"/publication/gis_chapter/","publishdate":"2019-12-07T00:00:00Z","relpermalink":"/publication/gis_chapter/","section":"publication","summary":"Forthcoming book chapter introducing GIS and geovisual analysis for research.","tags":["GIS","mapping","visualisation"],"title":"GIS and Geovisual Analysis","type":"publication"},{"authors":null,"categories":null,"content":"\r\rFrom reporting election results to issuing weather forecasts, maps offer a powerful, accessible and visually appealing way to convey complex information. Yet even the most beautiful maps can introduce some degree of misrepresentation.\nTo see how, consider the latest statistics on deprivation released by the UK government. The government ranked 32,844 neighbourhoods, based on measures of deprivation such as income, employment, health and crime. The figures were widely reported, from the BBC to The Guardian and the Daily Mail, reigniting long-standing debates about persistent inequality in England.\nOf course, many outlets used maps to share these findings with the public. But using traditional boundaries can divert readers’ attention away from important information. In BBC’s map of deprivation across local authorities, for instance, sparsely populated rural areas dominate a disproportionately large area, while urban areas, such as London, containing millions of people, are rendered almost invisible.\nThe map included in the BBC’s report on deprivation is interesting. It would be good to take a look at the Ofsted rating of schools in the most deprived areas... https://t.co/iUR2hINBRn pic.twitter.com/lFq6s8l1In\n\u0026mdash; SailingAway (@Heatherleatt) September 27, 2019  Distorted data\rRecent research shows that people can interpret information inaccurately, when they look at maps with these shortcomings. But other techniques can be used to create a more accurate impression of the underlying data. For instance, cartograms deliberately distort geography by scaling areas according to a specific variable, such as local population.\nDorling cartograms take this one step further, scaling areas according to a specified variable, but also representing each area using the same shape, such as a square or a circle. Other methods achieve uniformity in both size and shape: hexograms and geogrids transform the original boundaries of the map into hexagons or squares of the same size, while still aiming to preserve their original arrangement.\nMapping neighbourhood deprivation in England represents a significant challenge, even for experts. This is because the government defines a neighbourhood as a “lower super output area” – each of which contains around 1,500 residents.\nBecause highly deprived neighbourhoods tend to be densely populated, they are less visible on a regular map. By contrast, wealthier suburban areas – which are often less densely populated and therefore much larger – dominate the map. So there’s a risk that readers might draw inaccurate conclusions about the level of deprivation in any given area.\n\rShaping up\rUsing Dorling cartograms scaled by resident population, and regular hexagonal geogrids, I’ve attempted to minimise any misrepresentation. Take the example of Blackpool, which was ranked the most deprived local authority in England. Around 42% of neighbourhoods in Blackpool are in the top 10% most deprived in England (the “first decile”). Yet these neighbourhoods only make up around 29% of the city’s actual area.\nUsing a Dorling cartogram scaled by resident population size, neighbourhoods in the first decile now take up 41% of the map’s area, and the larger, less deprived neighbourhoods (in light blue) have been shrunk accordingly to become less dominant. With a regular hexagonal geogrid, the map now mimics the underlying data, with neighbourhoods in the first decile of deprivation taking up 42% of the total area.\nThe variation in the shapes and sizes of Blackpool’s neighbourhoods can no longer divert attention or mislead readers, and arguably, the underlying data has been conveyed more accurately than with the original map.\n\rMapping deprivation differently\rA striking difference can be seen using different types of maps to visualise deprivation in Burnley and Hartlepool, which were among England’s most deprived local authorities. Both areas are characterised by many small, densely populated neighbourhoods in the first decile (most deprived), surrounded by much larger and wealthier suburbs.\nAt first glance, deprivation appears understated in the map that uses the original boundaries. But with the Dorling cartogram (again scaled by population) and the hexagonal geogrids, the issue can be somewhat rectified. This comes with its own difficulties, though.\nThe geogrid in particular pushed some neighbourhoods further apart and forced some closer together than they appear on the original map. This might prove problematic in cases where the primary aim of the map is to convey distinct geographic patterns, such as the clustering of highly deprived neighbourhoods. In making the transformation, these patterns might be lost, or spurious patterns generated.\n\rEncouraging experimentation\rBirmingham and Manchester – ranked in the top ten most deprived local authorities – present a different challenge. With much larger populations, and many more neighbourhoods to visualise, the original map presents an overwhelming amount of information.\nOnce again, both the Dorling cartogram and the geogrid perform admirably, scaling down the visual impact of larger, wealthier areas (especially in the north of Birmingham) and making the maps easier to read – without skewing the geographic patterning of deprivation across each city.\nThere are no firm rules about how best to create beautiful but accurate maps. In this case, using the latest neighbourhood deprivation data in England, mapping out the original boundaries can clearly introduce some degree of misrepresentation. There’s real value in experimenting with different cartographic techniques – and media organisations and the government would do well to think outside the box when it comes to sharing the latest findings with the public.\n\r","date":1573084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573084800,"objectID":"653da9ad6fc526bc9198fe6804c5874d","permalink":"/post/maps_conversation/","publishdate":"2019-11-07T00:00:00Z","relpermalink":"/post/maps_conversation/","section":"post","summary":"From reporting election results to issuing weather forecasts, maps offer a powerful, accessible and visually appealing way to convey complex information. Yet even the most beautiful maps can introduce some degree of misrepresentation.","tags":["visualisation","maps","deprivation","neighbourhoods","dorling","cartograms"],"title":"Even the most beautiful maps can be misleading","type":"post"},{"authors":["Samuel Langton","Reka Solymosi"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"9dcefe63ddbc3c8d5071a2871fe44c6a","permalink":"/publication/hexo_epb/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/hexo_epb/","section":"publication","summary":"Paper exploring the extent to which different methods of visualising area-based data can remedy (or exacerbate) misrepresentation by presenting results from a crowdsourced survey.","tags":["visualisation","maps","brexit","cartograms","hexograms"],"title":"Cartograms, hexograms and regular grids. Minimising misrepresentation in spatial data visualisations","type":"publication"},{"authors":null,"categories":null,"content":"\r\rOn a day-to-day basis, the exposure citizens have to the police is often fleeting, with officers passing by in a blur as they respond to emergency calls. Official crime figures can be disputed, but the long-term trend appears to be that levels of crime in England and Wales are heading downward. Yet it is a myth to think that an apparent drop in crime relieves some of the pressures placed on police forces.\nIn reality, the demands placed on police forces do not originate primarily from obvious criminal behaviour. Forces are increasingly burdened with what’s called “latent” crime demand, such as child sexual exploitation or modern slavery, where victims are afraid or unwilling to approach police, as well as non-crime issues, such as missing persons.\nThe volume of this demand, and the time spent on addressing it, is much more problematic to estimate, and yet the safety of the public depends on both the police and the government having a handle on such estimations.\nOut on the streets, police forces need to demonstrate an understanding of current and future demand to allocate resources efficiently and ensure calls are responded to in time. In Westminster, the funding allocations to the 43 forces of England and Wales are dictated by estimations of demand. In an era of austerity, measuring this demand has never been more crucial.\nMeasuring demand\rReports by the College of Policing and HM Inspectorate of Constabulary (HMIC), have shown how the realities of modern policing render traditional estimates of demand outdated and inaccurate.\nIn its annual review of policing in England and Wales, HMIC rates how well each police force estimates the current and future demand for their services. For their latest report, published in March 2017 and covering inspections between February 2016 and March 2017, the message is clear: most forces are doing a good job, but there is room for improvement.\nThe map below, generated using open-source spatial data, plots the latest force ratings. It paints a positive picture for England and Wales, but nine out of 43 police forces require improvement, and only a handful managed to achieve the top grade.\nA key component of improving police awareness of their demand will be refining the understanding of “latent” and non-crime demand. Latent demand often involves vulnerable people who are unwilling to come forward due to the nature of the crime, or live in communities with a widespread distrust of the police. It is therefore extremely difficult to make reliable estimations about this kind of demand on police time.\nA substantial proportion of the demands placed on police forces originate from non-crime issues such as mental health. Although estimates vary, some forces estimate that 70% to 80% of their demand is comprised of non-crime incidents. A number of forces believe that the increase in non-criminal incidents being dealt with by the police may be partially driven by cuts to other areas of the public sector.\n\rMoney depends on it\rTo ensure that police forces are allocated money appropriately, in the interests of public safety and economic efficiency, the government also needs to demonstrate an understanding of demand. The issue of police funding remains a highly charged one, and budget cuts were keenly debated during the 2017 election campaign.\nThe map here uses recent police funding figures from 2017-2018 to demonstrate the vast sums of public money involved, and how much these sums can vary between police force areas\nDespite the implications and public concern over police funding, refining the formula used to apportion money has been fraught with difficulties. A December 2015 report by MPs on the Home Affairs Committee demonstrated the problems the government has faced when attempting to estimate demand for the purposes of funding allocations. It detailed how a freshly proposed police funding formula, which aimed to simplify the estimation process, was almost immediately put on hold following the realisation that incorrect data was being used for the calculation. Although a new government consultation on a revised funding calculation was launched in September 2016, its outcome has not yet been announced.\n\rThe role of experts\rAcademic researchers are often justifiably scolded for not engaging with real-world discussions, and failing to offer practical solutions to contemporary problems in society. Yet here they clearly have something to offer.\nHMIC reported that some police forces have made significant improvements to their demand estimations by working with academic research institutions who can provide much needed methodological expertise. But such collaborations are not widespread. Only recently has there been an acceptance that a concerted collaborative effort, including academic experts, is required to improve the government’s demand-based funding formula. Hopefully in this way past errors by the Home Office can be rectified, and trust in the system restored.\nRecent history tells us that collaborations can be fruitful, and that there is the potential to drastically improve estimations of demand through such partnership. Academics, police forces and the government must bear the responsibility of instigating further change. The safety of the public may depend on it.\n\r","date":1499299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499299200,"objectID":"ae9ce2b981063f92af22ee8219c62696","permalink":"/post/crimedrop_conversation/","publishdate":"2017-07-06T00:00:00Z","relpermalink":"/post/crimedrop_conversation/","section":"post","summary":"On a day-to-day basis, the exposure citizens have to the police is often fleeting, with officers passing by in a blur as they respond to emergency calls. Official crime figures can be disputed, but the long-term trend appears to be that levels of crime in England and Wales are heading downward.","tags":["policing","crime","HMIC","funding"],"title":"When crime drops, the demands on the police don’t necessarily fall","type":"post"},{"authors":["Samuel Langton","Wouter Steenbeek"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"50f3051447846f23589fcad3f2b42874","permalink":"/publication/burg_ag/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/burg_ag/","section":"publication","summary":"Paper using Google Street View data to investigate the extent to which the physical attributes of residential homes contribute to the risk of burglary. Please feel free to contact me for a copy if you do not have access!","tags":["burglary","google street view","crime","rational choice","situational crime prevention"],"title":"Residential burglary target selection. An analysis at the property-level using Google Street View","type":"publication"}]