<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>r | Samuel Langton</title>
    <link>/tag/r/</link>
      <atom:link href="/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>r</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 08 Apr 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu347c2c227070d4d829f6f2e71f22c263_2514_512x512_fill_lanczos_center_3.png</url>
      <title>r</title>
      <link>/tag/r/</link>
    </image>
    
    <item>
      <title>Wrangling freedive watch data in R</title>
      <link>/post/freedives/</link>
      <pubDate>Sat, 08 Apr 2023 00:00:00 +0000</pubDate>
      <guid>/post/freedives/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Towards the end of 2021, I finally plucked up the courage to take a &lt;a href=&#34;https://www.padi.com/education/freediving&#34;&gt;freediving&lt;/a&gt; course. At the time, I had never even tried Scuba diving, but a love of swimming and a childish love of trying to swim pool lengths underwater, combined with several &lt;a href=&#34;https://www.youtube.com/watch?v=2-8g03IUY2k&#34;&gt;inspiring videos&lt;/a&gt;, was enough for me to give it a try. Since then, my diving has been limited to swimming pools in Amsterdam and very cold, dark Dutch lakes, together with a local dive group. But this year, I finally managed to get to Dahab, Egypt, to help prepare for the depth component of my &lt;a href=&#34;https://www.aidainternational.org/Education/AIDAFreedivingCourses#aida3&#34;&gt;AIDA3&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:photo&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/langtonhugh/freedives/main/img/photo_credited.png&#34; alt=&#34;On the ascent in Dahab. Photo taken by Pete Botman. Credit also to Pete for lending me his Amsterdam-themed fins!&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: On the ascent in Dahab. Photo taken by Pete Botman. Credit also to Pete for lending me his Amsterdam-themed fins!
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Freediving is pretty wholesome, and the gear-free component is part of what I enjoy so much, especially compared to Scuba. But before going to Dahab, I finally succumbed to buying a freedive watch (a Suunto D4F). The watch measures and records basic information about your dive, including important safety information such as your surface interval times. The inevitable problem is that the watch is fairly clumsy when it comes to viewing historical dives. I had all this awesome information on my dives locked away behind a black and white screen and a few buttons. I found the &lt;a href=&#34;https://www.suunto.com/en-nl/Support/software-support/dm5/&#34;&gt;Suunto desktop software&lt;/a&gt; pretty outdated and restrictive. Opening it up gave me very &lt;em&gt;Windows 95&lt;/em&gt; vibes (great vibes, but not what I am looking for here).&lt;/p&gt;
&lt;p&gt;So, I set about exploring whether the data could be pulled off the watch and summarised using R. The ultimate aim might be to create an app in which people can drag-and-drop their own data. But, let’s not carried away. For now, if you want to replicate what I show below, you can find everything on an open &lt;a href=&#34;https://github.com/langtonhugh/freedives&#34;&gt;GitHub repository&lt;/a&gt;. If you run the code on your own data, please let me know how you found it, and what I could add to improve things. This is very much an initial exploration.&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;The first thing you’ll need to do is load in a few packages. Nothing unusual here (mainly &lt;code&gt;tidyverse&lt;/code&gt;-based packages). The exception is &lt;code&gt;XML&lt;/code&gt; which we need to load in the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load packages.
library(XML)
library(dplyr)
library(tidyr)
library(stringr)
library(lubridate)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-and-parsing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading and parsing&lt;/h2&gt;
&lt;p&gt;Each dive is archived into its own XML file. This means that even just for this year, I have 102 individual XML files. This is rather annoying but easily resolved by pasting together the working directories of all of these files based on their extension (i.e., .xml). If you use the corresponding GitHub repository, and work within the project file, all of this will run nice and smoothly. Nested within &lt;code&gt;paste()&lt;/code&gt; is the &lt;code&gt;list.files()&lt;/code&gt; function which simply produces a character vector for all the XML files in the specified folder. You can use this approach for any file type you want.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# List files.
dive_files &amp;lt;- paste(&amp;quot;data/2023/&amp;quot;, list.files(&amp;quot;data/2023/&amp;quot;, pattern = glob2rx(&amp;quot;*.xml&amp;quot;),  recursive=TRUE), sep = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have all the working directories stored within the &lt;code&gt;dive_files&lt;/code&gt; list object, we can run the &lt;code&gt;xmlParse()&lt;/code&gt; function through the whole thing. This produces a list of XML documents, which we then further parse using &lt;code&gt;xmlToList()&lt;/code&gt;. The output &lt;code&gt;dive_list&lt;/code&gt; is a list of lists, but bear with me: we extract the useful information in one beautiful swoop in the next step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load data.
dive_xml_list &amp;lt;- lapply(dive_files, xmlParse)

# Parse into a list of lists.
dive_list &amp;lt;- lapply(dive_xml_list, xmlToList)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-pull-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The pull function&lt;/h2&gt;
&lt;p&gt;This step is basically the only challenging part of the process. I would love someone with more experience in wrangling XML files to improve this. At the moment, I run a function which, for each dive record, creates a &lt;code&gt;tibble&lt;/code&gt; containing the bits of information that we need, such as the date of the dive, the depth of dives, duration and temperature.&lt;/p&gt;
&lt;p&gt;The one thing that makes this odd, and involving &lt;em&gt;yet more lists&lt;/em&gt;, is that the watch measures depth and temperature at specified time intervals (a sampling rate) of two-seconds. Nevertheless, we can pull these out using &lt;code&gt;lapply()&lt;/code&gt; and then convert to a numeric vector within this &lt;code&gt;tibble()&lt;/code&gt; function. We also do some trivial extras, like convert the depth readings to a negative number. This function is the key to extracting your dive data in a usable &lt;em&gt;tidy&lt;/em&gt; format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function to pull out relevant information.
pull_fun &amp;lt;- function(x){
tibble(
    mode  = x[[&amp;quot;Mode&amp;quot;]],
    date  = x[[&amp;quot;StartTime&amp;quot;]],
    depth = as.numeric(lapply(x[[&amp;quot;DiveSamples&amp;quot;]], function(y){y$Depth})),
    time  = as.numeric(lapply(x[[&amp;quot;DiveSamples&amp;quot;]], function(y){y$Time})),
    max_depth   = as.numeric(x[[&amp;quot;MaxDepth&amp;quot;]]),
    depth_minus = depth*-1,
    max_depth_minus = max_depth*-1,
    duration  = as.numeric(x[[&amp;quot;Duration&amp;quot;]]),
    temp  = as.numeric(lapply(x[[&amp;quot;DiveSamples&amp;quot;]], function(y)y$Temperature))
  ) %&amp;gt;% 
    mutate(date = str_replace_all(date, &amp;quot;T&amp;quot;, &amp;quot; &amp;quot;),
           date_lub = ymd_hms(date),
           date_lub_min = round_date(date_lub, &amp;quot;minute&amp;quot;),
           date_day = round_date(date_lub, &amp;quot;day&amp;quot;))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the magic: running this function through the list(s), and sticking it all together into a single &lt;code&gt;tibble&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Run function through list.
dive_df_list &amp;lt;- lapply(dive_list, pull_fun)

# Bind together.
dive_info_df &amp;lt;- bind_rows(dive_df_list, .id = &amp;quot;dive_id&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case you are not running this as we go along, the output looks like our ol’ familiar data frames with rows and columns. The data frame is in long format: we have two-second samples nested within each dive id. This is just the first ten rows.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
dive_id
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
mode
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
date
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
depth
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
time
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
max_depth
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
depth_minus
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
max_depth_minus
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
duration
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
temp
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
date_lub
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
date_lub_min
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
date_day
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:09
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.58
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.999994
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:09
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:09
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.999994
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:09
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:09
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.70
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.70
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.999994
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:09
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:09
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.999994
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:09
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:18:00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.22
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.999994
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.27
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.27
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.999994
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.54
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.999994
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.999994
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:29
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:23:00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:27:32
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.32
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.90
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-7.32
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.999994
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:27:32
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:28:00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:27:32
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.32
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-7.32
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.999994
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:27:32
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15 11:28:00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2023-01-15
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;One thing I noticed here is that the maximum depth is often not recorded in one of the two-second samples. For example, in first dive (which was probably just a practice &lt;a href=&#34;https://www.youtube.com/watch?v=5oyfvvz51Fc&#34;&gt;duck dive&lt;/a&gt;), the max depth is 2.01 metres but none of the four samples include this depth measurement. I can only guess that the sampling rate is more frequent, but that the watch only saves the two-second intervals and the max depth.&lt;/p&gt;
&lt;p&gt;Anyway, as we can already see, not all of these 102 dives were ‘proper’ dives. I know for sure that two of them are Scuba dives, and many of them will be dives for which I was a &lt;a href=&#34;https://www.deeperblue.com/safety-for-freediving/&#34;&gt;safety buddy&lt;/a&gt;. The Scuba dives are easily identifiable by the ‘mode’. To filter out safety dives, I have simply subsetted the data for dives which were shallower than eight metres. This is completely arbitrary: some of my safety dives will have been deeper, and some of my ‘proper dive’ attempts will have been shallower, but you can choose whatever threshold you’d like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove SCUBA dives, and likely safety-buddy dives.
dive_info_clean_df &amp;lt;- dive_info_df %&amp;gt;%
  filter(mode == 3, max_depth &amp;gt; 8) %&amp;gt;% 
  mutate(dive_id = as.numeric(dive_id))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dive-profile&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dive profile&lt;/h2&gt;
&lt;p&gt;With the data in tidy format, things get easier and more fun. The main graphic I wanted to create is a ‘dive profile’ visual for which we plot the depth on the Y-axis and the duration on the X-axis using &lt;code&gt;ggplot2&lt;/code&gt;. In this plot, I colour the line according to the depth, which is measures at the two-second sampling rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot dives profile.
dive_info_clean_df %&amp;gt;% 
  ggplot(data = .) +
  geom_line(mapping = aes(x = time, y = depth_minus, group = dive_id, colour = depth_minus),
            size = 1, alpha = 1) +
  scale_colour_viridis_c() +
  scale_x_continuous(breaks = c(0, 15, 30, 45, 60, 75, 90, 105)) +
  theme_bw() +
  geom_hline(yintercept = 0, linetype = &amp;quot;dotted&amp;quot;) +
  labs(y = &amp;quot;Depth (metres)&amp;quot;, x = &amp;quot;Time (seconds)&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/langtonhugh/freedives/main/visuals/dive_profile.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Most of these dives will have been following a vertical rope for safety, as per the photo above. One downside of the dive profile graphic is that at first glance it might imply that the dives are covering distance along the X-axis. But, I think with the proper labeling it is a nice visual representation of the dives. We can easily differentiate outliers by both time (such as a hang at around 12 metres) and my two deepest dives (both are ~25 metres). Any other ideas, let me know. One thing I’d like to fix is the colouring by depth: at high resolution, we can see the segments for the sampling rate. I’d rather it was a smooth gradient colour. But, we go some way to remedy this later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;chronology&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chronology&lt;/h2&gt;
&lt;p&gt;To explore progression over time (which for this data, is only 2023), I create a chronological id variable based on the dates. The order of the dives is interesting, but not so much the date itself. Maybe there’s a nicer way of doing this, but it certainly made the plot later easier. The idea is to later plot max depth progression in chronological order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create chronological id and join back with the main data.
dive_info_clean_df &amp;lt;- dive_info_clean_df %&amp;gt;% 
  distinct(date_lub) %&amp;gt;% 
  mutate(chrono_id = 1:nrow(.)) %&amp;gt;% 
  right_join(dive_info_clean_df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then visualise, by each dive, the max depth using an (upside down) lollipop graphic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Basic chronology plot.
dive_info_clean_df %&amp;gt;% 
  distinct(chrono_id, max_depth_minus, .keep_all = TRUE) %&amp;gt;%
  ggplot(data = .) +
  geom_segment(mapping = aes(x = chrono_id, xend = chrono_id,
                             y = 0, yend = max_depth_minus)) +
  geom_point  (mapping = aes(x = chrono_id, y = max_depth_minus)) +
  geom_hline(yintercept = 0, linetype = &amp;quot;dotted&amp;quot;) +
  theme_bw() +
  labs(x = &amp;quot;Dive number (chronological)&amp;quot;, y = &amp;quot;Depth (metres)&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/langtonhugh/freedives/main/visuals/basic_chronology.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;This might prove useful in its own right, but it lacks the colour which perfectly (for me) represents the depth. The deeper you go, the darker it gets. I came up with a rather messy way of adding incremental depth measurements between zero metres (i.e., the surface) and the max depth (the turn on the line). I do this using the &lt;code&gt;seq()&lt;/code&gt; function, specifying a sequence of 0.5 metres. This is nested within &lt;code&gt;sapply()&lt;/code&gt; so that we generate the sequence for each dive record. The output is a list, so I then convert the sequence to a character, remove the &lt;code&gt;c(&lt;/code&gt; and &lt;code&gt;)&lt;/code&gt; sandwiching the sequence, and pivot to long format by splitting rows by the comma. The final step is to make each value negative again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Add incremental steps to the max_depths.
dive_sequence_df &amp;lt;- dive_info_clean_df %&amp;gt;% 
  mutate(depth_sequence = sapply(dive_info_clean_df$max_depth, function(x)seq(0, x, by = 0.5))) %&amp;gt;%
  select(chrono_id, depth_sequence) %&amp;gt;% 
  mutate(depth_sequence = as.character(depth_sequence),
         depth_sequence = str_sub(depth_sequence, 3, -2)) %&amp;gt;% 
  separate_rows(depth_sequence, sep = &amp;quot;,&amp;quot;) %&amp;gt;% 
  mutate(depth_sequence = -1*as.numeric(trimws(depth_sequence)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can reproduce the chronology plot but colour the line according to the sequence, with a minor addition to the basic &lt;code&gt;ggplot2&lt;/code&gt; code chunk. You’ll notice that I create a mini data frame beforehand which contains only the chrono id and the max depth measure. This lets me add (coloured) points to the end of each line, but perhaps there’s some smarter &lt;code&gt;ggplot2&lt;/code&gt; code that would avoid the pre-step.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Identify the max depth for each chrono id.
max_depth_chrono_df &amp;lt;- dive_sequence_df %&amp;gt;% 
  group_by(chrono_id) %&amp;gt;% 
  summarise(max_depth_minus = min(depth_sequence))

# Plot the sequence graph again.
ggplot() +
  geom_line(data = dive_sequence_df, mapping = aes(x = chrono_id, y = depth_sequence, group = chrono_id,
                          colour = depth_sequence), size = 2) +
  geom_point(data = max_depth_chrono_df,
             mapping = aes(x = chrono_id, y = max_depth_minus, 
                           colour = max_depth_minus), size = 3) +
  geom_hline(yintercept = 0, linetype = &amp;quot;dotted&amp;quot;) +
  scale_colour_viridis_c() +
  theme_bw() +
  labs(x = &amp;quot;Dive number (chronological)&amp;quot;, y = &amp;quot;Depth (metres)&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/langtonhugh/freedives/main/visuals/improved_chronology.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Finally, we can summarise the distribution of max depth, along with the temperature of the water and duration of each dive. This step is pretty straightforward. The &lt;code&gt;pivot_longer()&lt;/code&gt; means that we can &lt;code&gt;facet_wrap()&lt;/code&gt; rather than create each plot individually.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Distribution handling and plot.
dive_info_clean_df %&amp;gt;% 
  select(dive_id, duration, max_depth, temp) %&amp;gt;% 
  distinct() %&amp;gt;% 
  rename(`Duration (seconds)` = duration,
         `Max. depth (metres)`  = max_depth,
         `Temperature (C)` = temp) %&amp;gt;% 
  pivot_longer(cols = -dive_id, names_to = &amp;quot;stat&amp;quot;) %&amp;gt;% 
  ggplot(data = .) +
  geom_density(mapping = aes(x = value), fill = &amp;quot;grey20&amp;quot;, colour = &amp;quot;grey20&amp;quot;) +
  facet_wrap(~stat, scales = &amp;quot;free&amp;quot;) +
  theme_bw() +
  labs(x = NULL, y = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/langtonhugh/freedives/main/visuals/enviro_density.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Well, that’s pretty much it for now! It’s a start. The code should work pretty seamlessly for anyone with a Suunto D4F, but the drag-and-drop dashboard might be a long way off yet. This is partly because I don’t know to what extent this code will run on dive logs from different brands and model of watch. If you get it running using your own data (irrespective of the watch model) let me know!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Misrepresentation in maps: mini-survey results from the Ministry of Defence</title>
      <link>/post/mod_blog/</link>
      <pubDate>Thu, 27 Jan 2022 00:00:00 +0000</pubDate>
      <guid>/post/mod_blog/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A couple of years ago, &lt;a href=&#34;https://rekadata.net/&#34;&gt;Reka Solymosi&lt;/a&gt; and I began a side-project on different ways of visualising spatial data. We were (well, still are) interested in how people interpret maps, and how these interpretations might differ depending the type of map being used, even when the underlying data is the same. We were recently invited by the UK’s &lt;a href=&#34;https://www.gov.uk/government/organisations/ministry-of-defence&#34;&gt;Ministry of Defence&lt;/a&gt; to share our research experience in this area. As part of the presentation, we conducted a short survey of the MOD attendees to test how well people make estimates about spatial data when looking at different types of visualisation. This post provides a bit of background to the topic and reports on the findings from our survey. The maps, survey data and code to reproduce everything is &lt;a href=&#34;https://github.com/langtonhugh/spatial_viz&#34;&gt;openly available&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Thanks to various pieces of &lt;a href=&#34;https://eagereyes.org/blog/2016/a-reanalysis-of-a-study-about-square-pie-charts-from-2009&#34;&gt;clever research&lt;/a&gt; and &lt;a href=&#34;http://www.thefunctionalart.com/p/reviews.html&#34;&gt;fantastic books&lt;/a&gt;, we know that people can misinterpret data visualisations. Maps are no different. One reason why people can misinterpret maps is due to the size of the areas being visualised. Never have I seen more spatial visualisations flying around than during the COVID-19 pandemic: people were desperate to compare how different countries, regions or neighbourhoods were fairing, and maps are an accessible and beautiful way to convey this information. But, countries, regions and neighbourhoods can vary considerably in size. Take this map of England below. It was published by Public Health England and subsequently &lt;a href=&#34;https://www.bbc.com/news/uk-55086621&#34;&gt;reported by the BBC&lt;/a&gt; in November 2020. It shows the different lockdown tiers which were set to come into place before Christmas that year.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/phe.png&#34; title=&#34;Two maps showing before and after lockdown tiers across local authorities in England. The local authorities comprising London are small and difficult to see clearly.&#34; alt=&#34;Two maps showing before and after lockdown tiers across local authorities in England. The local authorities comprising London are small and difficult to see clearly.&#34; width=&#34;1000px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The problem with maps like this is that the areas being mapped (local authorities) are not uniform in size or shape. Densely populated local authorities, such as those that comprise London, are almost invisible, while less urbanised areas, which are geographically large, dominate the visual. It’s a rare occasion where London &lt;em&gt;not&lt;/em&gt; being the focus of attention is probably not a good thing, considering the number of people residing in these lockdown tiers. The makers of the map were probably well-intentioned: after all, they are using the real, raw boundaries of local authorities – but that doesn’t guarantee that the map will convey the underlying data clearly. In this particular case, it is reasonably likely that there isn’t much variation going on between these small, compact units – but of course, we cannot be certain by examining this map in isolation, and let’s be honest, not everyone is going to read the text accompanying a beautiful map.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;alternative-mapping-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Alternative mapping methods&lt;/h2&gt;
&lt;p&gt;While the government and media reporting of the pandemic gave us plenty of the above mapping examples, the issue is by no means uncommon, or particularly new. Just a few years earlier, maps were widely used to report on the result of the EU referendum in the UK (e.g. &lt;a href=&#34;https://www.bbc.com/news/uk-politics-36616028&#34;&gt;BBC&lt;/a&gt;). Largely due to the different voting behaviour between densely populated, urban London and the rest of the country, maps which visualised the result using raw local authority boundaries could be highly misleading. Geographically vast, leave-voting areas dominate the map, rendering the remain-strong London almost invisible. This also makes it more difficult for people to spot spatial patterns, such as like-for-like areas being geographically proximal to one another.&lt;/p&gt;
&lt;p&gt;This is where alternative forms of map can be useful. By deliberately distorting raw boundaries, or by assigning geographic areas to uniform shapes, we can actually improve the accuracy with which people interpret the data underlying a map. Back in 2019, in &lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/2399808319873923&#34;&gt;a paper together with Reka&lt;/a&gt;, we examined whether alternative types of map could more accurately convey the clustering of EU referendum remain voters in London compared to the original local authority boundaries (a pre-print of this paper is also freely available &lt;a href=&#34;https://osf.io/t6agd/download&#34;&gt;here&lt;/a&gt;). We did this using a sample of ~800 Reddit users. We found that people were &lt;em&gt;more likely&lt;/em&gt; to interpret information about the geographic patterning of the referendum result accurately when looking at two alternative types of map, balanced cartograms and hexograms, compared to the raw boundaries. But, other alternatives (hex and square grids) made people &lt;em&gt;less likely&lt;/em&gt; to accurately interpret the same information, compared to the raw boundaries. So, not all alternatives worked, but our findings made it clear that (1) mapping raw boundaries can be an inappropriate way of conveying spatial information, and (2) alternative mapping methods can (but not always) convey geographic clustering &lt;em&gt;better&lt;/em&gt; than raw boundaries. There were of course caveats to our study (e.g., generalisability), so we’d encourage you to read the full thing too!&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/eu_figure.png&#34; alt=&#34;Five maps visualising EU referendum results across local authorities in England using different methods: original boundaries, hexogram, hex grid and square grid. It shows how mapping out the raw boundaries can render small areas almost invisible, whereas the alternative maps make each areas equal in size.&#34; width=&#34;1000px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Proportion of remain voters in England at the local authority level, visualised using different mapping techniques. Source: Langton &amp;amp; Solymosi (2019).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mod-extension&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;MOD extension&lt;/h2&gt;
&lt;p&gt;Following our study, and an &lt;a href=&#34;https://methods.sagepub.com/video/minimizing-misrepresentation-in-spatial-data-visualizations&#34;&gt;awesome training video&lt;/a&gt; that Reka made in collaboration with SAGE, we were contacted by a scientific adviser at the UK’s Ministry of Defence to share our musings and findings on this topic to MOD personnel. While we didn’t know the precise role and background of the attendees, we knew that many had a military background, and had expertise in analytics and scientific research. You can find the slides for this on &lt;a href=&#34;https://rekadata.net/talks/mod_presentation.html#1&#34;&gt;Reka’s website&lt;/a&gt;. We took this opportunity to conduct another mini-survey using different data, with a slightly different focus, and of course, a very different sample of respondents.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;The broad motivation behind the mini-survey was comparable: Do people make &lt;strong&gt;worse&lt;/strong&gt; estimates about the underlying data when observing the &lt;strong&gt;original boundaries&lt;/strong&gt; of a map, in comparison to &lt;strong&gt;alternative methods&lt;/strong&gt; (e.g. hex maps)? To study this, we took the example of neighbourhood deprivation in England. This is a salient example of the issue associated with mapping small areas. Neighbourhoods in England–defined as Lower Super Output Areas–are designed to be uniform by population size (~1500 residents). But, deprived areas are much more likely to be densely populated, and thus geographically small, compared to wealthier neighbourhoods, which are often sparsely populated and therefore geographically large. As a result, even regions containing lots of deprived neighbourhoods might not, at first glance and with limited information, appear particularly deprived.&lt;/p&gt;
&lt;p&gt;Based on our previous study, we boldly thought: hey, using alternative mapping techniques we can &lt;em&gt;better convey the underlying data&lt;/em&gt; compared to the original neighbourhood boundaries. To test this, we obtained data on neighbourhood deprivation across three local authorities: Birmingham, Hartelpool and Burnley. As measured using the &lt;a href=&#34;https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/835115/IoD2019_Statistical_Release.pdf&#34;&gt;Index of Multiple Deprivation&lt;/a&gt; in 2019, these are some of the most deprived local authorities in England. For each region, we mapped out neighbourhood deprivation using three different mapping techniques: the raw boundaries, a hex grid (using the &lt;a href=&#34;https://github.com/jbaileyh/geogrid&#34;&gt;geogrid&lt;/a&gt; R package) and a dorling cartogram (using the &lt;a href=&#34;https://cran.r-project.org/web/packages/cartogram/cartogram.pdf&#34;&gt;cartogram&lt;/a&gt; package) scaled by resident population. Here’s Birmingham using these techniques. For simplification, we recoded the typical IMD score which runs from 1 (most deprived) to 10 (least deprived) to 1-5 (e.g., 1 and 2 were recorded to 1, and so on).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/example_birm.png&#34; title=&#34;Three maps of neighbourhood deprivation in Birmingham, each visualised using different methods, namely, the original boundaries, a hex grid and a dorling cartogram.&#34; alt=&#34;Three maps of neighbourhood deprivation in Birmingham, each visualised using different methods, namely, the original boundaries, a hex grid and a dorling cartogram.&#34; width=&#34;1000px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nine maps (three regions, each visualised three different ways) were shown to the MOD participants one after the other. To try and mitigate against respondents using the previous answer as a guide, the same regions were never shown consecutively. For each map, respondents were asked to estimate the percentage of residents living in the &lt;em&gt;most deprived&lt;/em&gt; neighbourhood (1 - dark blue). Of course, we knew the answer to this already, because we had the underlying data, but the respondents were asked to make this assessment with limited information. So, for each map, we would end up with a range of different estimates from respondents, which could then be compared to the true figure. The larger the degree of error between the estimates and the true number, the ‘worse’ the map was communicating the underlying data.&lt;/p&gt;
&lt;p&gt;Here, it’s worth noting that when we were discussing the survey findings during the MOD presentation, after it had been completed, a number of respondents questioned whether the colour of the first category was ‘dark blue’, but rather, a shade of purple. We return to this later. Aside from making me question my ability to distinguish different shades of blue-purple, it brought up another interesting discussion about visualisation and surveys more generally. We’ve shared a screenshot of the first page of the survey below.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:unnamed-chunk-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/img/survey_screenshot.png&#34; alt=&#34;A screenshot from the survey in MS Forms. It shows the first question from the survey and an example map, for which respondents must estimate the proportion of residents living in the most deprived neighbourhood.&#34; width=&#34;1000px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: A screenshot from the first page of the survey. The survey was created is MS Forms.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;Respondents had around 24 hours to complete the survey. In total, we had 70 responses.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Two respondents were dropped because they were tests. Two additional people were dropped for missing questions, and two more were dropped because they contained text answers. This gave us 64 completed surveys for analyses. You can re-create all the maps and analyses reported here using the code on the corresponding &lt;a href=&#34;https://github.com/langtonhugh/spatial_viz&#34;&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the graphic below, we plot the distribution of respondents’ estimates for each region, and each map type. The dotted line represents the &lt;strong&gt;true value&lt;/strong&gt; (reality). Of course, this true value is identical for each region, irrespective of the map type. A number of things emerge from this visual. Let’s start with the distribution of estimates when people were observing the original boundaries. In all three study regions, people tended to &lt;em&gt;underestimate&lt;/em&gt; the percentage of residents living in the most deprived category. This is exactly what we would expect: the most deprived neighbourhoods are geographically small, occupying a smaller proportion of the map, so with no other information available, people underestimate.&lt;/p&gt;
&lt;p&gt;Hex maps, on the other hand, which assign each neighbourhood polygon to a grid of hexagons, appear to improve the accuracy of people’s estimates. Generally speaking, respondents got closer to the true figure when viewing the hex map compared to the original boundaries. Particularly with Burnley and Hartlepool, there is a noticeable peak in the distribution close to the true value. This is quite expected: the hexagons, which are uniformly sized and shaped, reflect the similarly uniform residential size of neighbourhoods. In this way, they are quite similar to &lt;a href=&#34;https://r-charts.com/part-whole/waffle-chart-ggplot2/&#34;&gt;waffle charts&lt;/a&gt; but arranged to reflect the geography of the study region. That said, we know from our &lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/2399808319873923&#34;&gt;previous study&lt;/a&gt; using the EU referendum data, and a visual inspection of the maps used here, that hex grids can massively distort the spatial distribution of polygons. So, while the hex maps used here have some clear benefits when it comes to interpreting aggregate information, the spatial patterning itself might be lost along the way. Interestingly, there is no such peak in respondent estimates around the true figure for the Birmingham hex map. This might be a result of the sheer number of neighbourhoods compared to Burnley and Hartlepool, or because there was less variation in the size of neighbourhoods nested within the city&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/mod_blog_files/figure-html/unnamed-chunk-5-1.png&#34; title=&#34;Nine different density plots showing the distribution of respondents&#39; estimates, one for each map in the survey.&#34; alt=&#34;Nine different density plots showing the distribution of respondents&#39; estimates, one for each map in the survey.&#34; width=&#34;1000px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When observing the dorling maps, people often slightly &lt;em&gt;overestimated&lt;/em&gt; the percentage of people residing in the most deprived neighbourhood. It’s not immediately clear why this is the case (at least, not to me), but this overestimation was pretty common across all three regions. Even though Lower Super Output Areas are designed to be fairly similar in residential size, there is still some variability, which is reflected in the differently sized circles representing each neighbourhood. Interestingly, this was the only map type which actually conveyed data about residential size. But, without any additional information or clarification (e.g., an extra legend), the dorling maps appear to have misled respondents fairly consistently across regions.&lt;/p&gt;
&lt;p&gt;The plot below visualises the same data slightly differently. Here, we plot the difference between each respondents’ estimate and the true figure, jittered slightly along the y-axis to avoid overlap between points. A boxplot summarises the overall spread. This does a reasonable job at demonstrating the degree of error in respondents’ estimates. But, it also shows the considerable spread in responses – making this estimate is not easy, and respondents clearly sometimes make completely erroneous guesses, possibly due to the survey design. In this particular case, the spread is minimised (overall) when using hex maps with a relatively small number of areas, as is the case with Hartlepool and Burnley.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/mod_blog_files/figure-html/unnamed-chunk-6-1.png&#34; title=&#34;Nine stacked boxplots, one for each map in the survey, showing the spread of error in respondents&#39; estimates (i.e., the difference between the estimated value and the true value.&#34; alt=&#34;Nine stacked boxplots, one for each map in the survey, showing the spread of error in respondents&#39; estimates (i.e., the difference between the estimated value and the true value.&#34; width=&#34;1000px&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;The findings from our mini-survey of MOD participants raise a number of interesting points – but also plenty of further questions. First, we found pretty good evidence to suggest that, in the absence of detailed information, visualising raw boundaries can misrepresent the data underlying a map. Here, respondents tended to &lt;em&gt;underestimate&lt;/em&gt; the proportion of residents living in highly deprived neighbourhoods when observing a map of the raw boundaries. This is precisely what we would expect due to their small geographic size in comparison to wealthier neighbourhoods. Instead, by assigning neighbourhoods to a hex grid, which are uniform in size and shape, respondents were able to make fairly accurate estimates. But, this benefit comes at a cost, namely, the distortion of spatial patterning. The dorling cartogram introduced the opposite effect as the original boundaries, with people tending to slightly &lt;em&gt;overestimate&lt;/em&gt; the overall proportion. So, alternative methods are not always ‘better’ but are certainly worth considering. Ultimately, it just depends on the aim of the research, and the message you want to convey with the visualisation.&lt;/p&gt;
&lt;p&gt;Conducting the mini-survey itself brought up an interesting learning point on survey design for these kinds of questions. As we noted earlier, many respondents questioned the description of the ‘most deprived’ category as ‘dark blue’. We used a colourblind friendly palette (viridis), but clearly, referring to colours by name in isolation is problematic. Different people will interpret the same colours differently, even with colourblind friendly palettes like viridis. You can try this out for yourself online using the &lt;a href=&#34;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&#34;&gt;viridis documentation&lt;/a&gt;. Fortunately, because we referred to the category by both colour and number, I think we largely avoided total confusion, otherwise we’d expect a clear bi-modal distribution in answers, or many ‘zero’ answers. Nevertheless, for a full-scale survey with a robust design and a generalisable sample, we would want to pilot the survey thoroughly beforehand to weed-out issues like this early on.&lt;/p&gt;
&lt;p&gt;Speaking of full-scale surveys: of course, mini-surveys like this are fun and interesting, and as noted, make useful learning experiences, but there is a huge ‘generalisability’ question over the results. We don’t know much about our MOD sample, so we cannot even generalise to the organisation itself. Any future work which aspires to generalisation might want to consider what populations are interesting and useful, and which map types (and for what purposes) are most relevant for that population. If anyone has pointers, suggestions or feedback, please do get in touch!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;It’s worth noting that, for the results presented in the MOD meeting, respondents only had around 8 hours. This gave us enough time to throw the R code together before the presentation. So, for the presentation we had 45 usable responses. The results between that the those presented here are very similar.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Akmedoids R package for generating directionally-homogeneous clusters of longitudinal data sets</title>
      <link>/publication/akmedoid/</link>
      <pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/publication/akmedoid/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Update: lockdown crime trends</title>
      <link>/post/update_lockdown/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/update_lockdown/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;font size=4.9&gt;&lt;/p&gt;
&lt;p&gt;Over the past few weeks I have spent a bit of time exploring police recorded crime trends before and after the UK-wide lockdown.&lt;/p&gt;
&lt;p&gt;There has been talk of lockdowns representing the &lt;a href=&#34;https://link.springer.com/article/10.1007/s12103-020-09546-0&#34;&gt;largest criminological experiment in history&lt;/a&gt;. Of course, determining the impact of this ‘experiment’ using police recorded crime data is not straightforward. Lots of parameters will have changed beyond people’s routine activities, amongst them, people’s ability and willingness to report crimes to the police, and policing resource allocation. It might be some time before we can fully understand how lockdowns have curbed the spatial and temporal patterns of crime.&lt;/p&gt;
&lt;p&gt;In the meantime, &lt;a href=&#34;https://data.police.uk/&#34;&gt;open&lt;/a&gt; police recorded crime data can offer insight into how criminal behaviour might have changed in the past few months.&lt;/p&gt;
&lt;div id=&#34;visualising-trends&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualising trends&lt;/h2&gt;
&lt;p&gt;A recent &lt;a href=&#34;https://theconversation.com/lockdown-crime-trends-why-antisocial-behaviour-is-up-140479&#34;&gt;article&lt;/a&gt; of mine explored how crime trends changed in March and April following the lockdown using London as a case study. It used a descriptive visualisation which was inspired by those used in &lt;a href=&#34;https://www.ft.com/content/a26fbf7e-48f8-11ea-aeb3-955839e06441&#34;&gt;the FT&lt;/a&gt; to track coronavirus deaths whilst accounting for seasonal changes in death rates. A key characteristic of long-term crime trends is seasonality. To disentangle fluctuations in crime, and try to pinpoint irregularity resulting from lockdown measures, we must account for typical monthly fluctuations. Even then, monthly data is &lt;a href=&#34;https://the-sra.org.uk/SRA/Blog/whyyoucantidentifychangesincrimebycomparingthismonthtolastmonth.aspx&#34;&gt;far from ideal&lt;/a&gt;, but you can’t have everything (especially when working from your living room).&lt;/p&gt;
&lt;p&gt;We now have some updated data – up to and including May, when some lockdown rules were relaxed – which demonstrates the start of a turnaround in crime trends. The code to make this visual is now available on &lt;a href=&#34;https://github.com/langtonhugh/crime_covid_visual&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/full_met.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The dramatic declines in burglary, robbery, theft and shoplifting observed in March and April have begun to reverse, or at least stabilise. Bicycle theft has more or less returned to normal when compared with previous years. As lockdown restrictions have eased, opportunities for crime have begun to (re)emerge. Counts are still unusually low amongst many crime types, but we are certainly witnessing a turnaround.&lt;/p&gt;
&lt;p&gt;Given that we can attribute most ASB incidents to &lt;a href=&#34;https://www.bbc.co.uk/news/uk-52298016&#34;&gt;lockdown breaches&lt;/a&gt;, it is no surprise to see counts beginning to decline. It will be interesting to see where this goes in June and July. We might see a resurgence of ‘traditional’ ASB amidst a fall in incidents associated with lockdown breaches, which will complicate the picture somewhat. We will be able to disentangle this through the analysis of secure records which provide some context to incidents (e.g. text logs), but the continued closure of universities (and thus, many secure data facilities) might slow this down.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;drug-activity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Drug activity&lt;/h2&gt;
&lt;p&gt;One of the most interesting talking points is around drugs. Initially, some reports suggested that drug use would &lt;a href=&#34;https://www.theguardian.com/society/2020/may/03/coronavirus-crisis-could-increase-users-drug-habits-report&#34;&gt;rise&lt;/a&gt; during lockdown, partly due to the resultant strains on mental health. Now, there are reports that usage, at least for party drugs like ecstasy and cocaine, has &lt;a href=&#34;https://www.theguardian.com/society/2020/jun/29/fall-in-use-of-party-drugs-as-more-britons-turn-to-alcohol-in-lockdown&#34;&gt;declined&lt;/a&gt;. We might then have expected a fall in drug offences, which includes possession, consumption and/or supply. Instead, we’ve seen an unprecedented increase throughout April and May.&lt;/p&gt;
&lt;p&gt;It is perfectly plausible that drug activity, such as those relating to &lt;a href=&#34;https://www.nationalcrimeagency.gov.uk/what-we-do/crime-threats/drug-trafficking/county-lines&#34;&gt;‘County Lines’&lt;/a&gt;, has &lt;a href=&#34;https://www.theguardian.com/uk-news/2020/apr/13/gangs-still-forcing-children-into-county-lines-drug-trafficking-police-covid-19-lockdown&#34;&gt;not changed much&lt;/a&gt;, or &lt;a href=&#34;https://shoc.rusi.org/informer/part-2-how-covid-19-affecting-county-lines&#34;&gt;declined&lt;/a&gt;, even amidst an increase in the number of offences being recorded by the police. Some forces reported an increase in arrests simply due to dealers &lt;a href=&#34;https://www.bbc.co.uk/news/uk-england-sussex-52344249&#34;&gt;sticking out&lt;/a&gt; on empty streets. This has been described as like &lt;a href=&#34;https://twitter.com/gmhales/status/1249962083631980549&#34;&gt;shooting fish in a barrel&lt;/a&gt; for police, and represents a good example of how trends in police recorded crime – however useful – should be interpreted carefully.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The monthly release of open police crime data in the UK offers a useful on-the-fly opportunity to examine how the lockdown may have impacted on crime. I’ll be updating this visual periodically upon new data releases to see how things are changing. Please do &lt;a href=&#34;https://www.samlangton.info/#contact&#34;&gt;get in contact&lt;/a&gt; if you have any suggestions or comments so far.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reads&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Further reads&lt;/h2&gt;
&lt;p&gt;Ashby, M. (2020). Changes in police calls for service during the early months of the 2020 coronavirus pandemic. &lt;a href=&#34;https://osf.io/preprints/socarxiv/h4mcu/&#34;&gt;Pre-print&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Campedelli, G. M., Aziani, A., &amp;amp; Favarin, S. (2020). Exploring the effect of 2019-nCoV containment policies on crime: The case of los angeles. &lt;a href=&#34;https://osf.io/gcpq8/&#34;&gt;Pre-print&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Gerell, M. (2020). Minor covid-19 association with crime in Sweden, a five week follow up. &lt;a href=&#34;https://osf.io/preprints/socarxiv/w7gka/&#34;&gt;Pre-print&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Halford, E., Dixon, A., Farrell, G., Malleson, N., &amp;amp; Tilley, N. (2020). Crime and coronavirus: social distancing, lockdown, and the mobility elasticity of crime. Crime Science, 9(1), 1-12.&lt;/p&gt;
&lt;p&gt;Stickle, B., &amp;amp; Felson, M. (2020). Crime Rates in a Pandemic: the Largest Criminological Experiment in History. American Journal of Criminal Justice, 1-12.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R</title>
      <link>/project/r-teaching/</link>
      <pubDate>Sat, 27 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/project/r-teaching/</guid>
      <description>&lt;p&gt;I have taught a number of courses using R and RStudio, including the development of teaching material both on my own and with colleagues.&lt;/p&gt;
&lt;h2 id=&#34;nscr&#34;&gt;NSCR&lt;/h2&gt;
&lt;p&gt;Co-organise an (ongoing) series of R training workshops at the 
&lt;a href=&#34;https://nscr.nl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NSCR&lt;/a&gt;. You can read more about our activities on the corresponding 
&lt;a href=&#34;https://nscrweb.netlify.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;. This includes leading a two-day Data Carpenties workshop, the material for which is available on 
&lt;a href=&#34;https://github.com/langtonhugh/nscr_carpentries&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;royal-statistical-society-conference&#34;&gt;Royal Statistical Society conference&lt;/h2&gt;
&lt;p&gt;Led an introductory session to the &lt;code&gt;tidyverse&lt;/code&gt; at the Royal Statistical Society (RSS) 2021 conference, along with colleagues from the University of Manchester. Material is available on 
&lt;a href=&#34;https://github.com/langtonhugh/rss_r&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;data-visualisation-in-r-using-crime-data&#34;&gt;Data visualisation in R using crime data&lt;/h2&gt;
&lt;p&gt;1-day workshop on data visualisation and mapping at the 
&lt;a href=&#34;https://www.cmi.manchester.ac.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cathie Marsh Institute&lt;/a&gt; (University of Manchester) in collaboration with the 
&lt;a href=&#34;https://ukdataservice.ac.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UK Data Service&lt;/a&gt;. Material can be found on the 
&lt;a href=&#34;https://ukdataservice.ac.uk/news-and-events/eventsitem/?id=5552&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UK Data Service website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The course was subsequently updated and re-ran online with the 
&lt;a href=&#34;https://ukdataservice.ac.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UK Data Service&lt;/a&gt; and 
&lt;a href=&#34;https://www.methods.manchester.ac.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Methods@Manchester&lt;/a&gt;. All material is available on 
&lt;a href=&#34;https://github.com/langtonhugh/data_viz_R_workshop&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;r-for-police-forces&#34;&gt;R for police forces&lt;/h2&gt;
&lt;p&gt;1-day workshop on data wrangling and visualisation for West Midlands Police Force. Please get in contact for material.&lt;/p&gt;
&lt;h2 id=&#34;r-for-criminologists&#34;&gt;R for criminologists&lt;/h2&gt;
&lt;p&gt;1-day workshop on data wrangling, visualisation and mapping hosted at the 
&lt;a href=&#34;https://www.esc-eurocrim.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;European Society of Criminology&lt;/a&gt; with colleagues from the 
&lt;a href=&#34;http://www.space-place-crime.eu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Space Place and Crime working group&lt;/a&gt;. Material is available on 
&lt;a href=&#34;https://github.com/langtonhugh/ESC2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and 
&lt;a href=&#34;https://rpubs.com/spaceplacecrime&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rpubs&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;getting-started-in-r---an-introduction-to-data-analysis-and-visualisation&#34;&gt;Getting Started in R - an introduction to data analysis and visualisation&lt;/h2&gt;
&lt;p&gt;4-day comprehensive introductory course at the 
&lt;a href=&#34;https://www.methods.manchester.ac.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Methods@Manchester&lt;/a&gt; summer school, co-taught with colleagues. Material is available as a 
&lt;a href=&#34;https://rcatlord.github.io/GSinR/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;cluster-analysis-in-r&#34;&gt;Cluster analysis in R&lt;/h2&gt;
&lt;p&gt;1-day workshop introducing cluster analysis in R at the 
&lt;a href=&#34;https://www.cmi.manchester.ac.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cathie Marsh Institute&lt;/a&gt; (University of Manchester), co-taught with a colleague. Please get in contact for material.&lt;/p&gt;
&lt;h2 id=&#34;r-for-social-scientists&#34;&gt;R for social scientists&lt;/h2&gt;
&lt;p&gt;I am a qualified instructor for 
&lt;a href=&#34;https://carpentries.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Data and Software Carpentries&lt;/a&gt;, and supported colleagues in teaching the 
&lt;a href=&#34;https://datacarpentry.org/r-socialsci/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;social science workshop&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;a-beginners-guide-to-statistics-for-criminology-and-criminal-justice-using-r&#34;&gt;A Beginner’s Guide to Statistics for Criminology and Criminal Justice Using R&lt;/h2&gt;
&lt;p&gt;Co-authored a textbook introducing R for criminology students, published by Springer. Available from 
&lt;a href=&#34;https://link.springer.com/book/10.1007/978-3-030-50625-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Springer&amp;rsquo;s website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
